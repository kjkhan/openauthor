{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install keras gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataset\n",
    "Load the [Reuter 50_50 training dataset](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50).\n",
    "\n",
    "TODO:  download and extract directly from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Drugstore giant Revco D.S. Inc. said Monday it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Mattel Inc., seeking to expand in the market f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>A financial agreement between Barney's Inc and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>An independent shareholder advisory firm recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Raising the stakes in the escalating battle fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text\n",
       "0  RobinSidel  Drugstore giant Revco D.S. Inc. said Monday it...\n",
       "1  RobinSidel  Mattel Inc., seeking to expand in the market f...\n",
       "2  RobinSidel  A financial agreement between Barney's Inc and...\n",
       "3  RobinSidel  An independent shareholder advisory firm recom...\n",
       "4  RobinSidel  Raising the stakes in the escalating battle fo..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source modified from:\n",
    "# https://github.com/devanshdalal/Author-Identification-task/blob/master/learner.py\n",
    "path = 'data/C50/C50train/'\n",
    "authors = os.listdir(path)\n",
    "data = []\n",
    "\n",
    "for author in authors:\n",
    "  texts = os.listdir(path + author + '/')\n",
    "  for text in texts:\n",
    "    f=open(path + author + '/' + text, 'r')\n",
    "    data.append([author, f.read()])\n",
    "    f.close()\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"author\", \"text\"])\n",
    "df.head()\n",
    "\n",
    "# TODO: add more author, text pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "### Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=20000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(df[\"text\"])\n",
    "X = tokenizer.texts_to_sequences(df[\"text\"])\n",
    "\n",
    "# plot histogram \n",
    "num_words = [len(n) for n in X]\n",
    "plt.hist(num_words, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the first text tokenized, size 500:\n",
      " [  203    64     9  1864    10     1    41    27    14   694     8    47\n",
      "    57 10446  1881   235   203    64   224 10446    80  1297    40   307\n",
      "    89   272    15  1133   216     4    64     9     1    36   393    80\n",
      "   250   201  8975    84    42    15   505    40  1487   565   272     5\n",
      "  1364   180     9  1316    13 10446   112   860  4841  2325     6   462\n",
      "     6    87  6499    37   115   868  1032   120   933   288   954   764\n",
      "     6   256   120    87   470   391    72  5156    34  1743    22   579\n",
      "     1   933    41     2  2096   264    59     4  1432   764     3   861\n",
      " 17281     7   201   682     7    12    22   311    13  4011     2  1506\n",
      "     4  3542  1199   234   171    72   528  7482  3754   201   682    24\n",
      "    46  2640   523    10    25  5157     1  6500  4592     3  1708   719\n",
      "  2139    70     6   582  2266  1578    12    24    46  1691    16 12794\n",
      "   123   801     6    87     6    17   156    25  1940   645   558    71\n",
      "  1099   191  4011    11   197  2326     2  5744   125     5     1  3543\n",
      "    16   546  1381     3   201  8975 17282  1278   739     6  1767  6781\n",
      "   861     5  5745  4011    22    14   543     2  5320   201   682   676\n",
      "  1208     7  4842 14659     3  4979   518 14660    74   201  8975   123\n",
      "   801   814     2  7863    55    28   294     2  5746     5  7864     1\n",
      "   701     3   273  6211   137   775    16  1708  2139  1382   397    12\n",
      "  1780 17283     5     4    36   195   239     2   389  1507   123   801\n",
      "   118  3671    13     1    41    25     7    17  1960  3486  1278   739\n",
      "    11   284    15   141   509    26     3   689    54     7    45     3\n",
      " 10446    63  2097    22    14     2  5511  1069  5747   913     5  5745\n",
      "   229  4011  2195  3231   861     6   201   682    24  2747   861   633\n",
      "    43   265     1  1278     6     1   861   868    12    22    14     4\n",
      "   530   123   739     8  4011     7   610  5158     3   185  2793   301\n",
      "  5158   112    74  4011    49     7    12    22  5748     6  2748    17\n",
      "  2225   224     8   201   682    84   362  2920   235     1   224    35\n",
      "    46   211     2  7865   198     1    61    70    49     7    90  3357\n",
      "   136   214    22    14  5512    53    60  7866    52   498  4011   228\n",
      "    34   184  1079     4  5321   333    13   201   682   619     4 17284\n",
      "    15    17  2466     6  3544  6782   157    12   751     1   201   682\n",
      "   367  1289    59     1  1455     6  2166     1    36     6   244   224\n",
      "    20  4011   685     3    72  1104    42    32  1424   641   397 11469\n",
      "    68    19  1979    18     4   470  3004 11469   347     2   442   201\n",
      "   682   109  2195 14661  1209  1725     1  7867   131   163  4011  2195\n",
      "    66  4843   861     5   378 14662  7867     6  2140   163     1   107\n",
      "    13   201   682    19   344    18     4   731   454     8  4011    31\n",
      "   228    34    28    19   927     5     4   928   987    13  4711  1959\n",
      "     1    61    70    35  1079     4  1210     8  4711  1959     2   287\n",
      "  4011     8    39   161    44    23     1   107  3927    53   843  1606\n",
      "  1053    10     1   441    27   746   425   137]\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences, use 500 as maximum length.\n",
    "X = pad_sequences(X, maxlen=500)\n",
    "\n",
    "print(\"Here is the first text tokenized, size {}:\\n\".format(len(X[0])), X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author RobinSidel is one-hot encoded as:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded = encoder.fit_transform(df[\"author\"])\n",
    "y = to_categorical(encoded)\n",
    "\n",
    "print(\"Author {} is one-hot encoded as:\\n\".format(df[\"author\"][0]), y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New:  500 text from 10 authors\n",
      "Train: 1400 text from 40 authors\n",
      "Test:  600 text from 40 authors\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keeps some authors aside for hash testing\n",
    "x_train, x_new, y_train, y_new = train_test_split(X, y, train_size=0.8, shuffle=False)\n",
    "\n",
    "# Split remainder into 70% training and 30% testing and shuffle\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, train_size=0.7, random_state=1)\n",
    "\n",
    "print(\"New:  {} text from {} authors\".format(x_new.shape[0], len(np.unique(y_new, axis=0))))\n",
    "print(\"Train: {} text from {} authors\".format(x_train.shape[0], len(np.unique(y_train, axis=0))))\n",
    "print(\"Test:  {} text from {} authors\".format(x_test.shape[0], len(np.unique(y_test, axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data, showing authors\n",
      "[ 6 39 40 45  3 46 23  3  5  8  1 10 20 41 25 49 21 15 14  7 10 13 28 22\n",
      "  0 39 20 14 25 22 40  7 35  5 33  8 33 29 35 38  1 28 16 40 15 10 27  9\n",
      " 23 13  5 46 41 27 10 33  5 40 45  9 10 18 20 45 21 29 45 36 31 41 21  0\n",
      " 14 18 40 27 12 33 19 41 20  8 23 13  8 27 34 14 16 27 22 16 19 28 16 15\n",
      " 10 17 16 36]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample training data, showing authors\")\n",
    "print(np.argmax(y_train, axis=1)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                1650      \n",
      "=================================================================\n",
      "Total params: 2,582,258\n",
      "Trainable params: 2,582,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, RepeatVector\n",
    "import keras\n",
    "\n",
    "RUN_NAME = \"run 4 with LSTM 128, embed 128, batch_size=10\"\n",
    "max_features = 20000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, name='embed'))\n",
    "model.add(LSTM(32, dropout=0.8, recurrent_dropout=0.2, name='lstm'))\n",
    "model.add(Dense(50, activation='softmax', name='dense'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/30\n",
      "1120/1120 [==============================] - 19s 17ms/step - loss: 3.9005 - acc: 0.0455 - val_loss: 3.8867 - val_acc: 0.1143\n",
      "Epoch 2/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 3.8360 - acc: 0.0616 - val_loss: 3.8166 - val_acc: 0.0179\n",
      "Epoch 3/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 3.7217 - acc: 0.0455 - val_loss: 3.7102 - val_acc: 0.0750\n",
      "Epoch 4/30\n",
      "1120/1120 [==============================] - 17s 16ms/step - loss: 3.6289 - acc: 0.0982 - val_loss: 3.6156 - val_acc: 0.1071\n",
      "Epoch 5/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 3.5035 - acc: 0.1545 - val_loss: 3.4953 - val_acc: 0.1750\n",
      "Epoch 6/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 3.3575 - acc: 0.1839 - val_loss: 3.3904 - val_acc: 0.1714\n",
      "Epoch 7/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 3.2056 - acc: 0.2241 - val_loss: 3.2824 - val_acc: 0.1857\n",
      "Epoch 8/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 2.9926 - acc: 0.2768 - val_loss: 3.1555 - val_acc: 0.2393\n",
      "Epoch 9/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 2.8313 - acc: 0.3009 - val_loss: 3.0405 - val_acc: 0.2286\n",
      "Epoch 10/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 2.6458 - acc: 0.3509 - val_loss: 2.9049 - val_acc: 0.2714\n",
      "Epoch 11/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 2.4466 - acc: 0.4232 - val_loss: 2.8368 - val_acc: 0.2893\n",
      "Epoch 12/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 2.3260 - acc: 0.4571 - val_loss: 2.7575 - val_acc: 0.3000\n",
      "Epoch 13/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 2.1583 - acc: 0.4982 - val_loss: 2.6947 - val_acc: 0.3107\n",
      "Epoch 14/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 2.0012 - acc: 0.5545 - val_loss: 2.6363 - val_acc: 0.3214\n",
      "Epoch 15/30\n",
      "1120/1120 [==============================] - 17s 16ms/step - loss: 1.8773 - acc: 0.5875 - val_loss: 2.5684 - val_acc: 0.3357\n",
      "Epoch 16/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 1.7443 - acc: 0.6321 - val_loss: 2.5947 - val_acc: 0.3250\n",
      "Epoch 17/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 1.6312 - acc: 0.6643 - val_loss: 2.5357 - val_acc: 0.3429\n",
      "Epoch 18/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 1.5236 - acc: 0.6821 - val_loss: 2.4975 - val_acc: 0.3429\n",
      "Epoch 19/30\n",
      "1120/1120 [==============================] - 17s 16ms/step - loss: 1.3982 - acc: 0.7152 - val_loss: 2.5123 - val_acc: 0.3250\n",
      "Epoch 20/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 1.3265 - acc: 0.7375 - val_loss: 2.4991 - val_acc: 0.3536\n",
      "Epoch 21/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 1.2645 - acc: 0.7384 - val_loss: 2.4996 - val_acc: 0.3643\n",
      "Epoch 22/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 1.1545 - acc: 0.7848 - val_loss: 2.4928 - val_acc: 0.3857\n",
      "Epoch 23/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 1.1012 - acc: 0.7982 - val_loss: 2.4119 - val_acc: 0.3607\n",
      "Epoch 24/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 1.0354 - acc: 0.8009 - val_loss: 2.4981 - val_acc: 0.3714\n",
      "Epoch 25/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 0.9638 - acc: 0.8179 - val_loss: 2.4708 - val_acc: 0.3714\n",
      "Epoch 26/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 0.9084 - acc: 0.8214 - val_loss: 2.4786 - val_acc: 0.3857\n",
      "Epoch 27/30\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 0.8602 - acc: 0.8464 - val_loss: 2.4985 - val_acc: 0.3750\n",
      "Epoch 28/30\n",
      "1120/1120 [==============================] - 17s 16ms/step - loss: 0.8147 - acc: 0.8455 - val_loss: 2.4927 - val_acc: 0.3786\n",
      "Epoch 29/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 0.7342 - acc: 0.8652 - val_loss: 2.5473 - val_acc: 0.3893\n",
      "Epoch 30/30\n",
      "1120/1120 [==============================] - 18s 16ms/step - loss: 0.7321 - acc: 0.8634 - val_loss: 2.5506 - val_acc: 0.3893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2e4c3e80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = keras.callbacks.TensorBoard(\n",
    "    log_dir='logs/{}'.format(RUN_NAME),\n",
    "    write_graph=True,\n",
    "    histogram_freq=5\n",
    ")\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          epochs=30,\n",
    "          validation_split=0.2,\n",
    "#          callbacks=[logger],\n",
    "          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 796us/step\n",
      "Test score: 2.616125742594401\n",
      "Test accuracy: 0.39666666587193805\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test, batch_size=200)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and compare hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_author(index):\n",
    "    one_hot = y_new[index]\n",
    "    i = np.argmax(one_hot)\n",
    "    return encoder.inverse_transform(i)\n",
    "\n",
    "def get_hash(text):\n",
    "    prediction = model.predict(text)\n",
    "    prediction = prediction[-1]\n",
    "    return prediction\n",
    "\n",
    "def get_similarity(hash1, hash2):\n",
    "    return float(cosine_similarity([hash1], [hash2]))\n",
    "\n",
    "x_hash = [get_hash(x) for x in x_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparision of text 0 and 2 \tfor same author MatthewBunce is: \t\t1.0\n",
      "Comparision of text 0 and 11 \tfor same author MatthewBunce is: \t\t1.0\n",
      "Comparision of text 0 and 60 \tfor authors MatthewBunce and ToddNissen is: \t0.834044337272644\n",
      "Comparision of text 0 and 110 \tfor authors MatthewBunce and PeterHumphrey is: \t0.759009063243866\n"
     ]
    }
   ],
   "source": [
    "def get_similarity_from_index(i, j):\n",
    "    return get_similarity(x_hash[i], x_hash[j])\n",
    "\n",
    "def print_similarity(i, j):\n",
    "    similarity = get_similarity_from_index(i, j)\n",
    "    \n",
    "    if get_author(i) == get_author(j):\n",
    "        print(\"Comparision of text {} and {} \\tfor same author {} is: \\t\\t{}\".format(\n",
    "             i, j, get_author(i), similarity))\n",
    "    else:\n",
    "        print(\"Comparision of text {} and {} \\tfor authors {} and {} is: \\t{}\".format(\n",
    "             i, j, get_author(i), get_author(j), similarity))\n",
    "    \n",
    "print_similarity(0,2)\n",
    "print_similarity(0,11)\n",
    "print_similarity(0,60)\n",
    "print_similarity(0,110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives  8273\n",
      "False positives 58658\n",
      "True negatives  53842\n",
      "False negatives 3977\n"
     ]
    }
   ],
   "source": [
    "true_positive, true_negative, false_positive, false_negative = 0,0,0,0\n",
    "margin = 0.8\n",
    "num_texts = len(x_new)\n",
    "\n",
    "for i in range(num_texts):\n",
    "    for j in range(min(i+1, num_texts), num_texts):\n",
    "        similarity = get_similarity_from_index(i, j)\n",
    "        \n",
    "        if similarity >= margin:\n",
    "            if get_author(i) == get_author(j):\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "        else:\n",
    "            if get_author(i) == get_author(j):\n",
    "                false_negative += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "\n",
    "print(\"True positives \", true_positive)\n",
    "print(\"False positives\", false_positive)\n",
    "print(\"True negatives \", true_negative)\n",
    "print(\"False negatives\", false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly identified text belonging to each author:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AlexanderSmith': {'correct': 524, 'incorrect': 701},\n",
       " 'BernardHickey': {'correct': 636, 'incorrect': 589},\n",
       " 'GrahamEarnshaw': {'correct': 952, 'incorrect': 273},\n",
       " 'KirstinRidley': {'correct': 567, 'incorrect': 658},\n",
       " 'LydiaZajc': {'correct': 1086, 'incorrect': 139},\n",
       " 'MatthewBunce': {'correct': 976, 'incorrect': 249},\n",
       " 'PeterHumphrey': {'correct': 982, 'incorrect': 243},\n",
       " 'SarahDavison': {'correct': 745, 'incorrect': 480},\n",
       " 'TimFarrand': {'correct': 649, 'incorrect': 576},\n",
       " 'ToddNissen': {'correct': 1156, 'incorrect': 69}}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparison just for the same author\n",
    "y_new_ints = np.unique(np.argmax(y_new, axis=1))\n",
    "y_new_authors = encoder.inverse_transform(y_new_ints)\n",
    "new_authors = {name:{\"correct\":0, \"incorrect\":0} for name in y_new_authors}\n",
    "\n",
    "for i in range(num_texts):\n",
    "    author_i = get_author(i)\n",
    "    \n",
    "    for j in range(min(i+1, num_texts), num_texts):\n",
    "        if author_i == get_author(j):\n",
    "            similarity = get_similarity_from_index(i, j)\n",
    "            if similarity >= margin:\n",
    "                new_authors[author_i][\"correct\"] += 1\n",
    "            else:\n",
    "                new_authors[author_i][\"incorrect\"] += 1\n",
    "\n",
    "print(\"Number of correctly identified text belonging to each author:\")             \n",
    "new_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[524, 636, 952, 567, 1086, 976, 982, 745, 649, 1156]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[new_authors[d][\"correct\"] for d in new_authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kjkhan/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x=y_new_authors,\n",
    "    y=[new_authors[d][\"correct\"] for d in new_authors],\n",
    "    name='Correct'\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x=y_new_authors,\n",
    "    y=[new_authors[d][\"incorrect\"] for d in new_authors],\n",
    "    name='Incorrect'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    title='Hash correctness using Margin=0.8',\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='jupyter-basic_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The chart above shows that the neural network does a great job of identifying the text written by Lydia and Todd and a good job identifying the text from Graham, Matthew, and Peter.  For the other authors, it does not perform as well; in two cases (Alexander and Kristin) it is wrong more than it is right.  Another issue is the large percentage of false positives when compared other author texts.  While some text from different authors may share certain characteristics, the goal of the algorithm is to maximize their differences.\n",
    "\n",
    "More work will coming in two areas:  (1) improving the network to have > 80% test accuracy (if possible) and improving the comparision algorithm's ability to differentiate texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
