{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept\n",
    "\n",
    "A proof-of-concept of the main \"originality score\" algorithm: preprocessing a sample paper, performing analytics, saving the document's hash, and returning a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataset\n",
    "Load the [Reuter 50_50 training dataset](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50).\n",
    "\n",
    "TODO:  download and extract directly from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Drugstore giant Revco D.S. Inc. said Monday it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Mattel Inc., seeking to expand in the market f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>A financial agreement between Barney's Inc and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>An independent shareholder advisory firm recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Raising the stakes in the escalating battle fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text\n",
       "0  RobinSidel  Drugstore giant Revco D.S. Inc. said Monday it...\n",
       "1  RobinSidel  Mattel Inc., seeking to expand in the market f...\n",
       "2  RobinSidel  A financial agreement between Barney's Inc and...\n",
       "3  RobinSidel  An independent shareholder advisory firm recom...\n",
       "4  RobinSidel  Raising the stakes in the escalating battle fo..."
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source modified from:\n",
    "# https://github.com/devanshdalal/Author-Identification-task/blob/master/learner.py\n",
    "path = 'data/C50/C50train/'\n",
    "authors = os.listdir(path)\n",
    "data = []\n",
    "\n",
    "for author in authors:\n",
    "  texts = os.listdir(path + author + '/')\n",
    "  for text in texts:\n",
    "    f=open(path + author + '/' + text, 'r')\n",
    "    data.append([author, f.read()])\n",
    "    f.close()\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"author\", \"text\"])\n",
    "df.head()\n",
    "\n",
    "# TODO: add more author, text pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "### Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'house':\n",
      " [-1.3345e-01  3.4688e-01  3.0748e-01 -2.1794e-03  7.1898e-01 -2.8725e-03\n",
      "  9.5989e-02  5.5276e-01  1.2153e-01 -2.6555e-01 -1.0277e+00  7.2278e-01\n",
      " -4.2767e+00 -9.0406e-02  1.1909e-01 -5.0647e-02 -3.3165e-01 -1.8213e-01\n",
      " -3.6218e-01  6.9813e-03  2.0147e-01 -2.9150e-01 -1.6417e-01 -2.8022e-01\n",
      "  5.4800e-01 -5.8081e-01  3.8146e-01 -5.5519e-01  1.6094e-01 -5.2039e-02\n",
      " -1.4798e-01  1.0892e-03 -2.6702e-01 -1.7885e-01  5.1449e-02  6.7434e-02\n",
      "  9.5654e-02  5.6137e-01  7.1208e-03  4.7000e-01 -3.1460e-01  1.0552e+00\n",
      "  5.2215e-01 -4.8432e-01  2.8615e-01  7.9474e-02  6.4211e-01  6.5274e-01\n",
      " -2.6493e-01 -8.9566e-02 -2.6298e-01 -3.4906e-01  3.3645e-02  2.1278e-01\n",
      " -1.0738e+00 -3.6867e-01  1.8473e-01  3.3821e-01  5.7516e-01  1.7559e-01\n",
      " -1.5436e-01  5.2836e-02 -9.8523e-02 -4.0975e-01 -8.5839e-02 -3.1527e-01\n",
      "  1.7936e-01 -2.0953e-01  6.6424e-01 -5.7412e-02  2.4528e-01 -2.2577e-01\n",
      " -3.3233e-01  2.1225e-01  2.3743e-01  1.3298e-01 -4.4889e-01  4.9577e-01\n",
      "  4.3360e-01  2.4248e-01  1.6624e+00  4.2981e-01 -4.8961e-01 -2.3809e-01\n",
      "  1.6583e-01 -4.9037e-01  3.6121e-01  8.0868e-01  5.0630e-01 -6.9646e-02\n",
      " -5.2503e-01 -7.9513e-03  5.3885e-01 -7.6658e-02 -2.5745e-01  6.0910e-01\n",
      "  4.5299e-01 -3.2974e-01 -5.1177e-01 -2.7013e-01]\n",
      "\n",
      "Similar words to 'house':\n",
      " [('room', 0.8465880155563354), ('home', 0.8294692635536194), ('party', 0.7633090019226074), ('going', 0.7493972778320312), ('out', 0.7491909861564636), ('office', 0.747423529624939), ('now', 0.7471632957458496), ('apartment', 0.7466973662376404), ('up', 0.7463924884796143), ('family', 0.743867039680481)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# https:/github.com/RaRe-Technologies/gensim-data\n",
    "# glove-twitter-25\n",
    "# word2vec-google-news-300\n",
    "info = api.info()\n",
    "embed_model = api.load(\"glove-twitter-100\")\n",
    "\n",
    "# print sample data\n",
    "print(\"Embedding for 'house':\\n\", embed_model.wv['house'])\n",
    "print(\"\\nSimilar words to 'house':\\n\", embed_model.most_similar(\"house\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model[w] for w in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(df[\"text\"])\n",
    "X = tokenizer.texts_to_sequences(df[\"text\"])\n",
    "\n",
    "# plot histogram \n",
    "num_words = [len(n) for n in X]\n",
    "plt.hist(num_words, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save Tokenizers\n",
    "with open('data/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author RobinSidel is one-hot encoded as:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded = encoder.fit_transform(df[\"author\"])\n",
    "y = to_categorical(encoded)\n",
    "\n",
    "print(\"Author {} is one-hot encoded as:\\n\".format(df[\"author\"][0]), y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100 word chucks\n",
    "WINDOW_SIZE = 200\n",
    "WINDOW_SPACING = 100\n",
    "X_chunks = []\n",
    "y_chunks = []\n",
    "\n",
    "def chunk(x, y):\n",
    "    x_chunk = []\n",
    "    y_chunk = []\n",
    "    \n",
    "    for i in range(0, len(x)-WINDOW_SIZE, WINDOW_SPACING):\n",
    "        x_chunk.append(x[i:i+WINDOW_SIZE])\n",
    "        y_chunk.append(y)\n",
    "    \n",
    "    return x_chunk, y_chunk\n",
    "\n",
    "for i, _ in enumerate(X):\n",
    "    xc, yc = chunk(X[i], y[i])\n",
    "    X_chunks += xc\n",
    "    y_chunks += yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2118"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_chunks = np.array(X_chunks)\n",
    "y_chunks = np.array(y_chunks)\n",
    "len(X_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the first text tokenized, size 500:\n",
      " [  203    64     9  1864    10     1    41    27    14   694     8    47\n",
      "    57 10446  1881   235   203    64   224 10446    80  1297    40   307\n",
      "    89   272    15  1133   216     4    64     9     1    36   393    80\n",
      "   250   201  8975    84    42    15   505    40  1487   565   272     5\n",
      "  1364   180     9  1316    13 10446   112   860  4841  2325     6   462\n",
      "     6    87  6499    37   115   868  1032   120   933   288   954   764\n",
      "     6   256   120    87   470   391    72  5156    34  1743    22   579\n",
      "     1   933    41     2  2096   264    59     4  1432   764     3   861\n",
      " 17281     7   201   682     7    12    22   311    13  4011     2  1506\n",
      "     4  3542  1199   234   171    72   528  7482  3754   201   682    24\n",
      "    46  2640   523    10    25  5157     1  6500  4592     3  1708   719\n",
      "  2139    70     6   582  2266  1578    12    24    46  1691    16 12794\n",
      "   123   801     6    87     6    17   156    25  1940   645   558    71\n",
      "  1099   191  4011    11   197  2326     2  5744   125     5     1  3543\n",
      "    16   546  1381     3   201  8975 17282  1278   739     6  1767  6781\n",
      "   861     5  5745  4011    22    14   543     2  5320   201   682   676\n",
      "  1208     7  4842 14659     3  4979   518 14660    74   201  8975   123\n",
      "   801   814     2  7863    55    28   294     2  5746     5  7864     1\n",
      "   701     3   273  6211   137   775    16  1708  2139  1382   397    12\n",
      "  1780 17283     5     4    36   195   239     2   389  1507   123   801\n",
      "   118  3671    13     1    41    25     7    17  1960  3486  1278   739\n",
      "    11   284    15   141   509    26     3   689    54     7    45     3\n",
      " 10446    63  2097    22    14     2  5511  1069  5747   913     5  5745\n",
      "   229  4011  2195  3231   861     6   201   682    24  2747   861   633\n",
      "    43   265     1  1278     6     1   861   868    12    22    14     4\n",
      "   530   123   739     8  4011     7   610  5158     3   185  2793   301\n",
      "  5158   112    74  4011    49     7    12    22  5748     6  2748    17\n",
      "  2225   224     8   201   682    84   362  2920   235     1   224    35\n",
      "    46   211     2  7865   198     1    61    70    49     7    90  3357\n",
      "   136   214    22    14  5512    53    60  7866    52   498  4011   228\n",
      "    34   184  1079     4  5321   333    13   201   682   619     4 17284\n",
      "    15    17  2466     6  3544  6782   157    12   751     1   201   682\n",
      "   367  1289    59     1  1455     6  2166     1    36     6   244   224\n",
      "    20  4011   685     3    72  1104    42    32  1424   641   397 11469\n",
      "    68    19  1979    18     4   470  3004 11469   347     2   442   201\n",
      "   682   109  2195 14661  1209  1725     1  7867   131   163  4011  2195\n",
      "    66  4843   861     5   378 14662  7867     6  2140   163     1   107\n",
      "    13   201   682    19   344    18     4   731   454     8  4011    31\n",
      "   228    34    28    19   927     5     4   928   987    13  4711  1959\n",
      "     1    61    70    35  1079     4  1210     8  4711  1959     2   287\n",
      "  4011     8    39   161    44    23     1   107  3927    53   843  1606\n",
      "  1053    10     1   441    27   746   425   137]\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences, use 500 as maximum length.\n",
    "X = pad_sequences(X, maxlen=500)\n",
    "\n",
    "print(\"Here is the first text tokenized, size {}:\\n\".format(len(X[0])), X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugstore\n",
      "giant\n",
      "revco\n",
      "d\n",
      "s\n",
      "inc\n",
      "said\n"
     ]
    }
   ],
   "source": [
    "# Break sequences into chuncks\n",
    "inv_map = {v: k for k, v in tokenizer.word_index.items()}\n",
    "for i in [7482, 584, 4011, 2324, 48, 109, 7]:\n",
    "    print(inv_map[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network\n",
    "### Create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New:  500 text from 10 authors\n",
      "Train: 1400 text from 40 authors\n",
      "Test:  600 text from 40 authors\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keeps some authors aside for hash testing\n",
    "x_train, x_new, y_train, y_new = train_test_split(X, y, train_size=0.8, shuffle=False)\n",
    "\n",
    "# Split remainder into 70% training and 30% testing and shuffle\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, train_size=0.7, random_state=1)\n",
    "\n",
    "print(\"New:  {} text from {} authors\".format(x_new.shape[0], len(np.unique(y_new, axis=0))))\n",
    "print(\"Train: {} text from {} authors\".format(x_train.shape[0], len(np.unique(y_train, axis=0))))\n",
    "print(\"Test:  {} text from {} authors\".format(x_test.shape[0], len(np.unique(y_test, axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data, showing authors\n",
      "[ 6 39 40 45  3 46 23  3  5  8  1 10 20 41 25 49 21 15 14  7 10 13 28 22\n",
      "  0 39 20 14 25 22 40  7 35  5 33  8 33 29 35 38  1 28 16 40 15 10 27  9\n",
      " 23 13  5 46 41 27 10 33  5 40 45  9 10 18 20 45 21 29 45 36 31 41 21  0\n",
      " 14 18 40 27 12 33 19 41 20  8 23 13  8 27 34 14 16 27 22 16 19 28 16 15\n",
      " 10 17 16 36]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample training data, showing authors\")\n",
    "print(np.argmax(y_train, axis=1)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (None, 32)                20608     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                1650      \n",
      "=================================================================\n",
      "Total params: 2,582,258\n",
      "Trainable params: 2,582,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, SimpleRNN, RepeatVector, TimeDistributed, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, 128, mask_zero=True, name='embed'))\n",
    "#model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3, return_sequences=True, name='lstm')))\n",
    "#model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3, return_sequences=False, name='lstm2')))\n",
    "model.add(LSTM(32, dropout=0.4, recurrent_dropout=0.4, return_sequences=False, name='lstm2'))\n",
    "model.add(Dense(50, activation='softmax', name='dense'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.6539 - acc: 0.0580 - val_loss: 3.6590 - val_acc: 0.0464\n",
      "Epoch 2/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.6392 - acc: 0.0616 - val_loss: 3.6447 - val_acc: 0.0714\n",
      "Epoch 3/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.6315 - acc: 0.0634 - val_loss: 3.6283 - val_acc: 0.1464\n",
      "Epoch 4/20\n",
      "1120/1120 [==============================] - 22s 19ms/step - loss: 3.6185 - acc: 0.0661 - val_loss: 3.6096 - val_acc: 0.1571\n",
      "Epoch 5/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.6079 - acc: 0.0786 - val_loss: 3.5924 - val_acc: 0.1429\n",
      "Epoch 6/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.5902 - acc: 0.1054 - val_loss: 3.5738 - val_acc: 0.1464\n",
      "Epoch 7/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.5654 - acc: 0.1196 - val_loss: 3.5498 - val_acc: 0.1571\n",
      "Epoch 8/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.5434 - acc: 0.1089 - val_loss: 3.5226 - val_acc: 0.1714\n",
      "Epoch 9/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.5164 - acc: 0.1304 - val_loss: 3.4993 - val_acc: 0.1786\n",
      "Epoch 10/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.4934 - acc: 0.1411 - val_loss: 3.4628 - val_acc: 0.1750\n",
      "Epoch 11/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.4737 - acc: 0.1437 - val_loss: 3.4301 - val_acc: 0.1929\n",
      "Epoch 12/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.4427 - acc: 0.1518 - val_loss: 3.4009 - val_acc: 0.1821\n",
      "Epoch 13/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.4060 - acc: 0.1536 - val_loss: 3.3680 - val_acc: 0.1893\n",
      "Epoch 14/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.3894 - acc: 0.1527 - val_loss: 3.3382 - val_acc: 0.1857\n",
      "Epoch 15/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.3557 - acc: 0.1696 - val_loss: 3.3070 - val_acc: 0.1929\n",
      "Epoch 16/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.3348 - acc: 0.1786 - val_loss: 3.2735 - val_acc: 0.2000\n",
      "Epoch 17/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.2935 - acc: 0.1795 - val_loss: 3.2464 - val_acc: 0.2071\n",
      "Epoch 18/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.2572 - acc: 0.1875 - val_loss: 3.2160 - val_acc: 0.2071\n",
      "Epoch 19/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.2356 - acc: 0.1893 - val_loss: 3.1902 - val_acc: 0.2179\n",
      "Epoch 20/20\n",
      "1120/1120 [==============================] - 21s 19ms/step - loss: 3.2076 - acc: 0.1911 - val_loss: 3.1688 - val_acc: 0.2143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a278249b0>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          epochs=20,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'data/1-lstm32-model.h5'\n",
    "model = keras.models.load_model(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12134/12134 [==============================] - 9s 737us/step\n",
      "Test score: 4.292130588247294\n",
      "Test accuracy: 0.08529751112821841\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compare hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_author(index):\n",
    "    return encoder.inverse_transform(y_hash[index])\n",
    "\n",
    "def get_hash(text):\n",
    "    return model.predict(text)\n",
    "\n",
    "def get_similarity(hash1, hash2):\n",
    "    return cosine_similarity(hash1, hash2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hash = get_hash(x_new)\n",
    "y_hash = [np.argmax(y_hot) for y_hot in y_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparision of text 0 and 0 \tfor same author MatthewBunce is: \t\t 1.0000001192\n",
      "Comparision of text 0 and 100 \tfor same author MatthewBunce is: \t\t 0.2801046968\n",
      "Comparision of text 0 and 200 \tfor same author MatthewBunce is: \t\t 0.1978359669\n",
      "Comparision of text 0 and 300 \tfor same author MatthewBunce is: \t\t 0.3308121264\n",
      "Comparision of text 0 and 400 \tfor same author MatthewBunce is: \t\t 0.0289533120\n",
      "Comparision of text 0 and 500 \tfor same author MatthewBunce is: \t\t 0.1593162715\n",
      "Comparision of text 0 and 600 \tfor same author MatthewBunce is: \t\t 0.0391608477\n",
      "Comparision of text 0 and 700 \tfor same author MatthewBunce is: \t\t 0.0487956963\n",
      "Comparision of text 0 and 800 \tfor same author MatthewBunce is: \t\t 0.2819902301\n",
      "Comparision of text 0 and 900 \tfor authors MatthewBunce and ToddNissen is: \t 0.0784679949\n",
      "Comparision of text 0 and 1000 \tfor authors MatthewBunce and ToddNissen is: \t 0.0443410091\n",
      "Comparision of text 0 and 1100 \tfor authors MatthewBunce and ToddNissen is: \t 0.0389439613\n",
      "Comparision of text 0 and 1200 \tfor authors MatthewBunce and ToddNissen is: \t 0.0239984654\n",
      "Comparision of text 0 and 1300 \tfor authors MatthewBunce and ToddNissen is: \t 0.1531248540\n",
      "Comparision of text 0 and 1400 \tfor authors MatthewBunce and ToddNissen is: \t 0.0472984388\n",
      "Comparision of text 0 and 1500 \tfor authors MatthewBunce and ToddNissen is: \t 0.0310181789\n",
      "Comparision of text 0 and 1600 \tfor authors MatthewBunce and ToddNissen is: \t 0.0708940178\n",
      "Comparision of text 0 and 1700 \tfor authors MatthewBunce and ToddNissen is: \t 0.4343051314\n",
      "Comparision of text 0 and 1800 \tfor authors MatthewBunce and ToddNissen is: \t 0.0333467983\n",
      "Comparision of text 0 and 1900 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.6443231702\n",
      "Comparision of text 0 and 2000 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.8243534565\n",
      "Comparision of text 0 and 2100 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.3474953771\n",
      "Comparision of text 0 and 2200 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.6022681594\n",
      "Comparision of text 0 and 2300 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.0960182846\n",
      "Comparision of text 0 and 2400 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.7285993695\n",
      "Comparision of text 0 and 2500 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.4491792023\n",
      "Comparision of text 0 and 2600 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.2591739595\n",
      "Comparision of text 0 and 2700 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.9029661417\n",
      "Comparision of text 0 and 2800 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.1606640816\n",
      "Comparision of text 0 and 2900 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.4334158301\n",
      "Comparision of text 0 and 3000 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.3258396089\n",
      "Comparision of text 0 and 3100 \tfor authors MatthewBunce and TimFarrand is: \t 0.1793327481\n",
      "Comparision of text 0 and 3200 \tfor authors MatthewBunce and TimFarrand is: \t 0.2077868432\n",
      "Comparision of text 0 and 3300 \tfor authors MatthewBunce and TimFarrand is: \t 0.0781975538\n",
      "Comparision of text 0 and 3400 \tfor authors MatthewBunce and TimFarrand is: \t 0.0665078834\n",
      "Comparision of text 0 and 3500 \tfor authors MatthewBunce and TimFarrand is: \t 0.6196750402\n",
      "Comparision of text 0 and 3600 \tfor authors MatthewBunce and TimFarrand is: \t 0.0456635766\n",
      "Comparision of text 0 and 3700 \tfor authors MatthewBunce and TimFarrand is: \t 0.0195892639\n",
      "Comparision of text 0 and 3800 \tfor authors MatthewBunce and TimFarrand is: \t 0.1036672443\n",
      "Comparision of text 0 and 3900 \tfor authors MatthewBunce and TimFarrand is: \t 0.4624611735\n",
      "Comparision of text 0 and 4000 \tfor authors MatthewBunce and TimFarrand is: \t 0.0604436472\n",
      "Comparision of text 0 and 4100 \tfor authors MatthewBunce and SarahDavison is: \t 0.0522982515\n",
      "Comparision of text 0 and 4200 \tfor authors MatthewBunce and SarahDavison is: \t 0.1524019241\n",
      "Comparision of text 0 and 4300 \tfor authors MatthewBunce and SarahDavison is: \t 0.1824784428\n",
      "Comparision of text 0 and 4400 \tfor authors MatthewBunce and SarahDavison is: \t 0.0425596088\n",
      "Comparision of text 0 and 4500 \tfor authors MatthewBunce and SarahDavison is: \t 0.1094886139\n",
      "Comparision of text 0 and 4600 \tfor authors MatthewBunce and SarahDavison is: \t 0.1604641378\n",
      "Comparision of text 0 and 4700 \tfor authors MatthewBunce and SarahDavison is: \t 0.3326659501\n",
      "Comparision of text 0 and 4800 \tfor authors MatthewBunce and SarahDavison is: \t 0.0596377812\n",
      "Comparision of text 0 and 4900 \tfor authors MatthewBunce and SarahDavison is: \t 0.5087054968\n",
      "Comparision of text 0 and 5000 \tfor authors MatthewBunce and SarahDavison is: \t 0.2723962963\n",
      "Comparision of text 0 and 5100 \tfor authors MatthewBunce and SarahDavison is: \t 0.0955976322\n",
      "Comparision of text 0 and 5200 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.2015803456\n",
      "Comparision of text 0 and 5300 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.7816353440\n",
      "Comparision of text 0 and 5400 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0733932257\n",
      "Comparision of text 0 and 5500 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0844426975\n",
      "Comparision of text 0 and 5600 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.2729109824\n",
      "Comparision of text 0 and 5700 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0182921607\n",
      "Comparision of text 0 and 5800 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.3185677826\n",
      "Comparision of text 0 and 5900 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.2578741312\n",
      "Comparision of text 0 and 6000 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.3514987528\n",
      "Comparision of text 0 and 6100 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.2730021775\n",
      "Comparision of text 0 and 6200 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0372923054\n",
      "Comparision of text 0 and 6300 \tfor authors MatthewBunce and BernardHickey is: \t 0.7940853238\n",
      "Comparision of text 0 and 6400 \tfor authors MatthewBunce and BernardHickey is: \t 0.1272461563\n",
      "Comparision of text 0 and 6500 \tfor authors MatthewBunce and BernardHickey is: \t 0.7944070697\n",
      "Comparision of text 0 and 6600 \tfor authors MatthewBunce and BernardHickey is: \t 0.2539787889\n",
      "Comparision of text 0 and 6700 \tfor authors MatthewBunce and BernardHickey is: \t 0.1159931421\n",
      "Comparision of text 0 and 6800 \tfor authors MatthewBunce and BernardHickey is: \t 0.1712017506\n",
      "Comparision of text 0 and 6900 \tfor authors MatthewBunce and BernardHickey is: \t 0.0427367948\n",
      "Comparision of text 0 and 7000 \tfor authors MatthewBunce and BernardHickey is: \t 0.2528609335\n",
      "Comparision of text 0 and 7100 \tfor authors MatthewBunce and BernardHickey is: \t 0.1263588369\n",
      "Comparision of text 0 and 7200 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0804690868\n",
      "Comparision of text 0 and 7300 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0480396971\n",
      "Comparision of text 0 and 7400 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0351082869\n",
      "Comparision of text 0 and 7500 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0197140183\n",
      "Comparision of text 0 and 7600 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0498533621\n",
      "Comparision of text 0 and 7700 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0463898703\n",
      "Comparision of text 0 and 7800 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0511959717\n",
      "Comparision of text 0 and 7900 \tfor authors MatthewBunce and KirstinRidley is: \t 0.1559891403\n",
      "Comparision of text 0 and 8000 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0325820260\n",
      "Comparision of text 0 and 8100 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0274011753\n",
      "Comparision of text 0 and 8200 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0692750067\n",
      "Comparision of text 0 and 8300 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.1051354036\n",
      "Comparision of text 0 and 8400 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0322855711\n",
      "Comparision of text 0 and 8500 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0439649820\n",
      "Comparision of text 0 and 8600 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0397939086\n",
      "Comparision of text 0 and 8700 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.1935331672\n",
      "Comparision of text 0 and 8800 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0387662202\n",
      "Comparision of text 0 and 8900 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0424280018\n",
      "Comparision of text 0 and 9000 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.1263093054\n",
      "Comparision of text 0 and 9100 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.2400652319\n",
      "Comparision of text 0 and 9200 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0288289487\n",
      "Comparision of text 0 and 9300 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0216575768\n",
      "Comparision of text 0 and 9400 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.1287061870\n",
      "Comparision of text 0 and 9500 \tfor authors MatthewBunce and LydiaZajc is: \t 0.0244255792\n",
      "Comparision of text 0 and 9600 \tfor authors MatthewBunce and LydiaZajc is: \t 0.0393109508\n",
      "Comparision of text 0 and 9700 \tfor authors MatthewBunce and LydiaZajc is: \t 0.0494813249\n",
      "Comparision of text 0 and 9800 \tfor authors MatthewBunce and LydiaZajc is: \t 0.1288701743\n",
      "Comparision of text 0 and 9900 \tfor authors MatthewBunce and LydiaZajc is: \t 0.1192891151\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_similarity(i, j):\n",
    "    similarity = get_similarity([x_hash[i]], [x_hash[j]])\n",
    "    similarity = float(similarity)\n",
    "    if get_author(i) == get_author(j):\n",
    "        print(\"Comparision of text {} and {} \\tfor same author {} is: \\t\\t\".format(\n",
    "             i, j, get_author(i)), end=' ')\n",
    "        print(\"{:0.10f}\".format(similarity))\n",
    "    else:\n",
    "        print(\"Comparision of text {} and {} \\tfor authors {} and {} is: \\t\".format(\n",
    "             i, j, get_author(i), get_author(j)), end=' ')\n",
    "        print(\"{:0.10f}\".format(similarity))\n",
    "\n",
    "for i in range(0, 10000, 100):\n",
    "    print_similarity(0, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-dad6ef07ab97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my_hash\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_hash\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mtrue_positive\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "true_positive, true_negative, false_positive, false_negative = 0,0,0,0\n",
    "margin = 0.2\n",
    "num_texts = len(x_new)\n",
    "\n",
    "for i in range(num_texts - 1):\n",
    "    similarity = get_similarity([x_hash[i]], x_hash)\n",
    "    \n",
    "    for j in range(i, num_texts):\n",
    "        if similarity[0][j] >= margin:\n",
    "            if y_hash[i] == y_hash[j]:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "        else:\n",
    "            if y_hash[i] == y_hash[j]:\n",
    "                false_negative += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "\n",
    "print(\"True positives \", true_positive)\n",
    "print(\"False positives\", false_positive)\n",
    "print(\"True negatives \", true_negative)\n",
    "print(\"False negatives\", false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly identified text belonging to each author:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AlexanderSmith': {'correct': 15869, 'incorrect': 713159},\n",
       " 'BernardHickey': {'correct': 8897, 'incorrect': 436199},\n",
       " 'GrahamEarnshaw': {'correct': 13727, 'incorrect': 519301},\n",
       " 'KirstinRidley': {'correct': 11192, 'incorrect': 569311},\n",
       " 'LydiaZajc': {'correct': 5742, 'incorrect': 213710},\n",
       " 'MatthewBunce': {'correct': 8930, 'incorrect': 326860},\n",
       " 'PeterHumphrey': {'correct': 30291, 'incorrect': 680737},\n",
       " 'SarahDavison': {'correct': 16374, 'incorrect': 682779},\n",
       " 'TimFarrand': {'correct': 9079, 'incorrect': 450282},\n",
       " 'ToddNissen': {'correct': 16713, 'incorrect': 524607}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparison just for the same author\n",
    "y_new_ints = np.unique(np.argmax(y_new, axis=1))\n",
    "y_new_authors = encoder.inverse_transform(y_new_ints)\n",
    "new_authors = {name:{\"correct\":0, \"incorrect\":0} for name in y_new_authors}\n",
    "margin = 0.8\n",
    "\n",
    "for i in range(num_texts - 1):\n",
    "    similarity = get_similarity([x_hash[i]], x_hash)\n",
    "    \n",
    "    for j in range(i, num_texts):\n",
    "        if y_hash[i] == y_hash[j]:\n",
    "            if similarity[0][j] >= margin:\n",
    "                new_authors[get_author(i)][\"correct\"] += 1\n",
    "            else:\n",
    "                new_authors[get_author(i)][\"incorrect\"] += 1\n",
    "\n",
    "print(\"Number of correctly identified text belonging to each author:\")             \n",
    "new_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~kjkhan/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x=y_new_authors,\n",
    "    y=[new_authors[d][\"correct\"] for d in new_authors],\n",
    "    name='Correct'\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x=y_new_authors,\n",
    "    y=[new_authors[d][\"incorrect\"] for d in new_authors],\n",
    "    name='Incorrect'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    title='Hash correctness using Margin={}'.format(margin),\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='jupyter-basic_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The chart above shows that the neural network does a great job of identifying the text written by Lydia and Todd and a good job identifying the text from Graham, Matthew, and Peter. For the other authors, it does not perform as well; in two cases (Alexander and Kristin) it is wrong more than it is right. Another issue is the large percentage of false positives when compared to other author texts. While some text from different authors may share certain characteristics, the goal of the algorithm is to maximize their differences.\n",
    "\n",
    "More work will be coming in two areas: (1) improving the network to have > 80% test accuracy (if possible) and improving the comparison algorithm's ability to differentiate texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"data/3-lstm32-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = keras.models.load_model(\"data/1-lstm32-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
