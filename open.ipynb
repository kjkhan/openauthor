{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model\n",
    "A second-generation model of the main \"originality score\" algorithm: preprocessing a sample paper, performing analytics, saving the document's hash, and returning a score.  Uses embedding to improve understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataset\n",
    "Load the [Reuter 50_50 training dataset](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50).\n",
    "\n",
    "TODO:  download and extract directly from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/C50/C50all/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KeithWeir</td>\n",
       "      <td>Shares in British media group EMAP Plc tumbled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KeithWeir</td>\n",
       "      <td>British media company EMAP said on Monday it e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KeithWeir</td>\n",
       "      <td>British television and newspaper group United ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KeithWeir</td>\n",
       "      <td>Newcastle United season ticket holders were re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KeithWeir</td>\n",
       "      <td>EMI, one of the world's top five music compani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author                                               text\n",
       "0  KeithWeir  Shares in British media group EMAP Plc tumbled...\n",
       "1  KeithWeir  British media company EMAP said on Monday it e...\n",
       "2  KeithWeir  British television and newspaper group United ...\n",
       "3  KeithWeir  Newcastle United season ticket holders were re...\n",
       "4  KeithWeir  EMI, one of the world's top five music compani..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source modified from:\n",
    "# https://github.com/devanshdalal/Author-Identification-task/blob/master/learner.py\n",
    "authors = os.listdir(DATASET_PATH)\n",
    "data = []\n",
    "\n",
    "for author in authors:\n",
    "  texts = os.listdir(DATASET_PATH + author + '/')\n",
    "  for text in texts:\n",
    "    f=open(DATASET_PATH + author + '/' + text, 'r')\n",
    "    data.append([author, f.read()])\n",
    "    f.close()\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"author\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "codeCollapsed": false,
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download 'punkt' if this is first time in notebook\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 tokens in text:\n",
      " ['shares', 'in', 'british', 'media', 'group', 'emap', 'plc', 'tumbled', 'about', 'three', 'percent', 'on', 'monday', 'after', 'the', 'company', 'said', 'it', 'was', 'losing', 'managing', 'director', 'david', 'arculus', 'to', 'television', 'and', 'newspaper', 'firm', 'united', 'news', '&', 'amp', ';', 'media', '.', 'arculus', ',', '50', ',', 'who', 'joined', 'emap', 'in', '1972', ',', 'will', 'move', 'to', 'united', 'news', 'on', 'april', '7', 'as', 'chief', 'operating', 'officer', '.', 'his', 'responsibilites', 'there', 'will', 'range', 'from', 'consumer', 'publishing', 'through', 'television', 'to', 'trade', 'magazines', 'and', 'exhibitions', '.', '``', 'david', 'arculus', 'has', 'demonstrated', 'a', 'sure', 'touch', 'as', 'a', 'builder', 'of', 'media', 'businesses', '.', 'his', 'experience', 'and', 'proven', 'record', 'make', 'him', 'a', 'valuable', 'addition']\n"
     ]
    }
   ],
   "source": [
    "y = df[\"author\"]\n",
    "\n",
    "# change text to lower case, replace new lines, and tokenize\n",
    "X = df[\"text\"].str.lower().replace('\\n', ' ')\n",
    "X = [nltk.word_tokenize(x) for x in X]\n",
    "print(\"First 100 tokens in text:\\n\", X[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters dataset vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters dataset\n",
      "Total words: 2902829\n",
      "Unique words: 53207\n"
     ]
    }
   ],
   "source": [
    "all_text = [y for x in X for y in x]\n",
    "print(\"Reuters dataset\")\n",
    "print(\"Total words: {}\".format(len(all_text)))\n",
    "print(\"Unique words: {}\".format(len(set(all_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Reuters words:\n",
      " ['the', ',', '.', 'to', 'of', 'a', 'in', 'and', 'said', \"'s\", \"''\", '``', 'on', 'for', 'that', 'is', 'it', 'with', '$', 'be', 'at', 'by', 'its', 'was', 'as', 'from', 'he', 'will', 'but', 'has', 'have', 'percent', 'would', 'are', 'million', 'not', 'which', 'an', 'year', '(', ')', 'we', 'this', 'company', 'had', 'they', 'new', 'were', 'market', 'china', 'billion', 'up', 'more', 'been', '--', 'one', 'also', 'or', 'about', 'after', 'last', 'analysts', 'than', 'their', 'over', 'some', 'u.s.', 'hong', 'there', 'kong', 'could', 'who', 'two', 'i', 'group', 'business', 'share', 'first', 'other', 'his', 'government', 'companies', 'industry', 'bank', 'if', 'stock', 'expected', 'into', 'out', 'years', 'sales', 'shares', 'analyst', 'told', 'chinese', 'no', 'when', 'all', 'people', ';']\n"
     ]
    }
   ],
   "source": [
    "# Create a vocab of the Reuters dataset, ordered by frequency (decending)\n",
    "f = nltk.FreqDist(all_text)\n",
    "reuters_most_common = [w for (w,_) in f.most_common()]\n",
    "print(\"Top 100 Reuters words:\\n\", reuters_most_common[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google's top 10,000 word vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Google words:\n",
      " ['the', 'of', 'and', 'to', 'a', 'in', 'for', 'is', 'on', 'that', 'by', 'this', 'with', 'i', 'you', 'it', 'not', 'or', 'be', 'are', 'from', 'at', 'as', 'your', 'all', 'have', 'new', 'more', 'an', 'was', 'we', 'will', 'home', 'can', 'us', 'about', 'if', 'page', 'my', 'has', 'search', 'free', 'but', 'our', 'one', 'other', 'do', 'no', 'information', 'time', 'they', 'site', 'he', 'up', 'may', 'what', 'which', 'their', 'news', 'out', 'use', 'any', 'there', 'see', 'only', 'so', 'his', 'when', 'contact', 'here', 'business', 'who', 'web', 'also', 'now', 'help', 'get', 'pm', 'view', 'online', 'c', 'e', 'first', 'am', 'been', 'would', 'how', 'were', 'me', 's', 'services', 'some', 'these', 'click', 'its', 'like', 'service', 'x', 'than', 'find']\n"
     ]
    }
   ],
   "source": [
    "# Load top 10,000 English words, according to Google\n",
    "#    Source:  https://github.com/first20hours/google-10000-english\n",
    "with open('data/google-10000-english-usa-no-swears.txt', 'r') as f:\n",
    "  google_most_common = f.read().replace('\\n', ' ')\n",
    "google_most_common = nltk.word_tokenize(google_most_common)\n",
    "print(\"Top 100 Google words:\\n\", google_most_common[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare vocabs and choose one for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "codeCollapsed": false,
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters unique words: 4390. Here's the top 100.\n",
      "[',', '.', \"'s\", \"''\", '``', '$', '(', ')', '--', 'u.s.', ';', '&', \"n't\", '1996', \"'\", 'corp.', '1997', 'inc.', '...', '10', 'tonnes', '1995', '20', \"'re\", 'pence', '30', 'bre-x', '1', 'deng', '50', 'mci', ':', 'newsroom', '15', 'tung', '100', 'boeing', 'wang', 'co.', '25', 'takeover', '12', 'francs', 'traders', 'yuan', \"'ve\", '40', 'labour', 'rival', 'conrail', 'margins', 'busang', 'airbus', 'shareholder', 'ltd.', 'nomura', '1995.', '14', 'cocoa', 'stg', 'jiang', '-', '11', 'handover', '1994', 'long-term', '1996/97', '300', '2', '60', 'klaus', 'speculation', 'uaw', 'crowns', 'privatisation', '1998', '1997.', 'regulators', '90', 'barrick', 'long-distance', 'thomson-csf', 'brokerage', '1996.', 'spokeswoman', '18', '200', '?', 'tonne', 'jumped', '171', 'csx', '31', 'rivals', '16', \"'ll\", '13', 'telecoms', '=', 'natwest']\n",
      "\n",
      "Google unique words: 4279. Here's the top 100.\n",
      "['pm', 'click', 'x', 're', 'info', 'ebay', 'dvd', 'website', 'v', 'description', 'non', 'k', 'y', 'reply', 'teen', 'photos', 'gay', 'thread', 'gallery', 'library', 'accessories', 'forums', 'dec', 'cart', 'feedback', 'blog', 'login', 'q', 'quote', 'girls', 'z', 'poker', 'browse', 'nov', 'files', 'rss', 'color', 'archive', 'faq', 'fun', 'feb', 'mar', 'url', 'aug', 'downloads', 'apr', 'tips', 'edit', 'lyrics', 'linux', 'jul', 'archives', 'jun', 'girl', 'google', 'html', 'advertise', 'oh', 'pics', 'contents', 'album', 'ny', 'eur', 'pdf', 'usr', 'jewelry', 'dc', 'mon', 'homepage', 'teens', 'pre', 'zip', 'lesbian', 'logo', 'score', 'ok', 'ms', 'fri', 'courses', 'hi', 'artist', 'mode', 'button', 'wed', 'male', 'custom', 'cnet', 'featured', 'female', 'eg', 'bed', 'ip', 'maps', 'tue', 'thank', 'pa', 'paypal', 'favorite', 'thu', 'anti']\n"
     ]
    }
   ],
   "source": [
    "# Look through the differences between the two vocabs\n",
    "unique_reuters_words = [x for x in reuters_most_common[:10000] if x not in google_most_common]\n",
    "print(\"Reuters unique words: {}. Here's the top 100.\".format(len(unique_reuters_words)))\n",
    "print(unique_reuters_words[:100])\n",
    "\n",
    "unique_google_words = [x for x in google_most_common if x not in reuters_most_common[:10000]]\n",
    "print(\"\\nGoogle unique words: {}. Here's the top 100.\".format(len(unique_google_words)))\n",
    "print(unique_google_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of these unique words, in order of frequency, to see if they are domain-specific.  For the Reuters vocab, some are punctuation, many are numbers, and the remainder are mostly domain-specific (international-business) related words, such as *privitisation, pre-tax*, and *conglomerate* or names such as *murdoch* and *monsanto*.  For the Google vocab, some are letters, some computer-related such as *forums* and *login*, and some are more general such as *color* and *thank*.\n",
    "\n",
    "Let's use the Google vocab and add punctuations and contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "# Extend common vocab to include punctuation + contractions\n",
    "from string import punctuation\n",
    "vocab = google_most_common + list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text and authors to network-ready input\n",
    "### Download embedding model to represent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "# https:/github.com/RaRe-Technologies/gensim-data\n",
    "# glove-twitter-25\n",
    "# word2vec-google-news-300\n",
    "info = api.info()\n",
    "embed_model = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "embed_size = embed_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'house':\n",
      " [-1.3345e-01  3.4688e-01  3.0748e-01 -2.1794e-03  7.1898e-01 -2.8725e-03\n",
      "  9.5989e-02  5.5276e-01  1.2153e-01 -2.6555e-01 -1.0277e+00  7.2278e-01\n",
      " -4.2767e+00 -9.0406e-02  1.1909e-01 -5.0647e-02 -3.3165e-01 -1.8213e-01\n",
      " -3.6218e-01  6.9813e-03  2.0147e-01 -2.9150e-01 -1.6417e-01 -2.8022e-01\n",
      "  5.4800e-01 -5.8081e-01  3.8146e-01 -5.5519e-01  1.6094e-01 -5.2039e-02\n",
      " -1.4798e-01  1.0892e-03 -2.6702e-01 -1.7885e-01  5.1449e-02  6.7434e-02\n",
      "  9.5654e-02  5.6137e-01  7.1208e-03  4.7000e-01 -3.1460e-01  1.0552e+00\n",
      "  5.2215e-01 -4.8432e-01  2.8615e-01  7.9474e-02  6.4211e-01  6.5274e-01\n",
      " -2.6493e-01 -8.9566e-02 -2.6298e-01 -3.4906e-01  3.3645e-02  2.1278e-01\n",
      " -1.0738e+00 -3.6867e-01  1.8473e-01  3.3821e-01  5.7516e-01  1.7559e-01\n",
      " -1.5436e-01  5.2836e-02 -9.8523e-02 -4.0975e-01 -8.5839e-02 -3.1527e-01\n",
      "  1.7936e-01 -2.0953e-01  6.6424e-01 -5.7412e-02  2.4528e-01 -2.2577e-01\n",
      " -3.3233e-01  2.1225e-01  2.3743e-01  1.3298e-01 -4.4889e-01  4.9577e-01\n",
      "  4.3360e-01  2.4248e-01  1.6624e+00  4.2981e-01 -4.8961e-01 -2.3809e-01\n",
      "  1.6583e-01 -4.9037e-01  3.6121e-01  8.0868e-01  5.0630e-01 -6.9646e-02\n",
      " -5.2503e-01 -7.9513e-03  5.3885e-01 -7.6658e-02 -2.5745e-01  6.0910e-01\n",
      "  4.5299e-01 -3.2974e-01 -5.1177e-01 -2.7013e-01]\n",
      "\n",
      "Similar words to 'house':\n",
      " [('room', 0.8465880155563354), ('home', 0.8294692039489746), ('party', 0.7633090615272522), ('going', 0.7493972182273865), ('out', 0.7491908669471741), ('office', 0.7474234700202942), ('now', 0.7471631765365601), ('apartment', 0.7466973662376404), ('up', 0.7463924884796143), ('family', 0.7438669800758362)]\n"
     ]
    }
   ],
   "source": [
    "# print sample data\n",
    "print(\"Embedding for 'house':\\n\", embed_model.wv['house'])\n",
    "print(\"\\nSimilar words to 'house':\\n\", embed_model.most_similar(\"house\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5018/5018 [02:25<00:00, 34.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded vector representation of some words of the first text:\n",
      " [array([ 0.36901  , -0.097805 ,  0.77436  , -0.66244  , -0.1459   ,\n",
      "       -0.92174  , -0.059589 , -0.74321  ,  0.1989   , -0.69029  ,\n",
      "       -0.29205  ,  0.017334 , -2.4598   ,  0.17755  ,  0.038644 ,\n",
      "        0.066692 ,  0.043657 , -0.094778 ,  0.074137 , -0.43868  ,\n",
      "        0.35035  , -0.59119  ,  0.06502  , -0.13868  , -0.87692  ,\n",
      "        0.83511  , -0.024205 ,  0.17247  ,  0.33899  , -0.074402 ,\n",
      "        0.54115  ,  0.25162  ,  0.10621  , -0.27057  ,  1.0452   ,\n",
      "        0.010004 , -0.43414  ,  0.1705   , -0.37303  , -1.3054   ,\n",
      "        0.68982  , -0.27434  ,  0.36169  ,  0.12466  ,  0.5389   ,\n",
      "        0.11821  ,  0.40161  , -0.1011   , -0.46405  ,  0.65829  ,\n",
      "       -0.041556 , -0.68439  , -0.57501  ,  0.89671  , -0.19499  ,\n",
      "        0.4853   ,  0.41231  ,  0.064372 ,  0.29773  , -0.61602  ,\n",
      "       -0.80376  , -0.76901  ,  0.079914 , -0.014108 ,  0.89144  ,\n",
      "        0.27667  ,  0.21634  ,  0.46204  , -0.26506  , -0.10666  ,\n",
      "       -0.36486  , -0.25452  ,  0.57118  , -0.39961  , -0.72164  ,\n",
      "       -0.15084  , -0.71091  , -1.0656   ,  1.1713   ,  0.068052 ,\n",
      "        0.98118  ,  0.0024661,  0.64111  ,  0.33216  , -0.84056  ,\n",
      "        0.28027  ,  0.81615  ,  0.78153  , -0.8436   , -0.26685  ,\n",
      "        0.41875  ,  0.037017 ,  0.74241  ,  0.03888  ,  0.46738  ,\n",
      "       -1.1469   , -0.18562  , -0.52105  , -0.5475   ,  0.42886  ],\n",
      "      dtype=float32), array([-0.091552,  0.55193 ,  0.6166  ,  0.52463 ,  0.58652 ,  0.20113 ,\n",
      "       -0.55163 , -0.30853 ,  0.24079 ,  0.059981,  0.27608 ,  0.56519 ,\n",
      "       -5.5562  , -0.036084, -0.17879 , -0.90797 , -0.13528 ,  0.55302 ,\n",
      "       -1.5412  , -0.27153 ,  0.045949, -0.27613 , -0.031604,  0.29257 ,\n",
      "        0.17782 , -0.45742 , -0.17257 , -0.25651 , -0.7679  ,  0.31588 ,\n",
      "       -0.36951 ,  0.57035 , -0.15254 ,  0.42258 ,  0.81202 , -0.15159 ,\n",
      "        0.39439 ,  0.41327 , -0.19144 , -0.45827 , -1.5744  , -0.25215 ,\n",
      "       -0.62427 ,  0.11973 ,  0.35804 , -0.037619, -0.07418 ,  0.38664 ,\n",
      "       -0.41619 ,  0.10846 , -0.066371, -0.62441 , -0.16464 ,  0.28154 ,\n",
      "       -0.47376 , -0.54616 , -0.1153  , -1.0872  , -0.32708 ,  0.17912 ,\n",
      "       -0.81835 ,  0.45268 , -0.71417 , -0.2947  ,  0.036828, -0.32437 ,\n",
      "        0.22164 , -0.46105 ,  0.24221 , -0.18038 , -0.073568,  0.07334 ,\n",
      "        0.011495, -0.050368, -0.010352,  0.39953 , -1.0918  ,  0.074331,\n",
      "        0.69542 , -0.28939 ,  1.7249  , -0.46104 ,  0.14066 ,  0.53448 ,\n",
      "        0.13176 ,  0.061815,  0.16462 , -0.093384,  0.12402 ,  0.39528 ,\n",
      "        0.83573 , -0.52682 ,  0.53516 , -0.31543 , -0.56494 ,  0.077237,\n",
      "        0.43076 , -0.26206 ,  1.01    ,  0.37481 ], dtype=float32), array([-3.6417e-01, -1.1706e-01, -1.8812e-01,  1.1086e-01, -7.4353e-01,\n",
      "       -2.7591e-01,  1.3667e-01, -1.0679e-01,  5.5799e-01, -8.0950e-01,\n",
      "        1.6071e-02, -3.3109e-01, -2.9874e+00, -4.4074e-01,  2.5717e-01,\n",
      "        1.7479e-03,  2.0283e-01, -9.4630e-01,  7.0069e-01,  1.4255e-01,\n",
      "        6.8194e-01, -1.8975e-01,  6.6472e-01, -1.2281e-01, -1.2880e-01,\n",
      "        4.0925e-01, -4.1363e-01, -4.7689e-01,  9.7069e-03,  8.0489e-02,\n",
      "        7.1459e-01, -2.3740e-01, -8.4072e-01,  4.9569e-01,  7.0245e-01,\n",
      "       -8.7639e-01,  7.0260e-01,  1.0854e+00,  4.0505e-01, -8.6441e-01,\n",
      "        1.7420e-01, -4.2500e-01, -2.2357e-01, -7.8992e-01,  2.9468e-01,\n",
      "        5.5273e-02,  1.1008e-01, -4.5742e-02, -2.9576e-01, -8.0851e-01,\n",
      "       -4.1850e-01,  4.9652e-01, -2.4032e-01, -2.2686e-01,  4.3483e-01,\n",
      "        5.8520e-02,  6.2077e-02, -2.4461e-01, -5.4740e-01,  1.4278e-01,\n",
      "       -5.3926e-01,  6.9038e-01,  4.2747e-01, -5.1408e-01, -7.7759e-02,\n",
      "        2.2098e-01, -7.6141e-01,  4.2633e-01,  4.4367e-01, -3.0973e-02,\n",
      "       -4.4185e-01, -2.7490e-01,  1.2905e-01,  7.8315e-01,  5.1440e-01,\n",
      "        5.9832e-02, -2.3438e-01,  3.1093e-01,  2.8569e-01,  3.9215e-01,\n",
      "        1.7589e+00, -4.9845e-02,  6.6712e-01,  1.1373e+00,  3.8324e-01,\n",
      "        6.9178e-01,  2.8321e-01, -2.2081e-01,  9.7867e-02,  2.8972e-01,\n",
      "       -7.9008e-02, -4.4270e-01, -5.9130e-01, -8.0359e-02, -3.5320e-01,\n",
      "        1.2390e-01,  4.2083e-02,  2.1356e-01, -5.1587e-01,  6.9688e-03],\n",
      "      dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# filter rare words and convert to vectors\n",
    "for i, doc in enumerate(tqdm(X)):\n",
    "    X[i] = [embed_model[word] for word in doc if word in vocab and word in embed_model]\n",
    "        \n",
    "print(\"Embedded vector representation of some words of the first text:\\n\", X[0][0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "Creating smaller windows of data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100 word chucks\n",
    "WINDOW_SIZE = 200\n",
    "WINDOW_SPACING = 50\n",
    "\n",
    "def chunk(x, y):\n",
    "    X_chunk = []\n",
    "    y_chunk = []\n",
    "    \n",
    "    for i in range(0, len(x)-WINDOW_SIZE, WINDOW_SPACING):\n",
    "        X_chunk.append(x[i:i+WINDOW_SIZE])\n",
    "        y_chunk.append(y)\n",
    "    \n",
    "    return X_chunk, y_chunk\n",
    "\n",
    "X_chunks = []\n",
    "y_chunks = []\n",
    "for i, _ in enumerate(X):\n",
    "    xc, yc = chunk(X[i], y[i])\n",
    "    X_chunks += xc\n",
    "    y_chunks += yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_chunks shape: (31591, 200, 100)\n",
      "y_chunks shape: (31591,)\n"
     ]
    }
   ],
   "source": [
    "X_chunks = np.array(X_chunks)\n",
    "y_chunks = np.array(y_chunks)\n",
    "print(\"X_chunks shape:\", X_chunks.shape)\n",
    "print(\"y_chunks shape:\", y_chunks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training, test, and \"new\" sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New:   6319 text from 10 authors\n",
      "Train: 22744 text from 41 authors\n",
      "Test:  2528 text from 41 authors\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keeps some authors aside for hash testing\n",
    "x_train, x_new, y_train, y_new = train_test_split(X_chunks, y_chunks, train_size=0.8, shuffle=False)\n",
    "\n",
    "# Split remainder into 70% training and 30% testing and shuffle\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, train_size=0.9, random_state=1)\n",
    "\n",
    "print(\"New:   {} text from {} authors\".format(x_new.shape[0], len(np.unique(y_new, axis=0))))\n",
    "print(\"Train: {} text from {} authors\".format(x_train.shape[0], len(np.unique(y_train, axis=0))))\n",
    "print(\"Test:  {} text from {} authors\".format(x_test.shape[0], len(np.unique(y_test, axis=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode labels (authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author KeithWeir is one-hot encoded as:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#encoder = LabelEncoder()\n",
    "#encoded = encoder.fit_transform(df[\"author\"])\n",
    "#y = to_categorical(encoded)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded = encoder.fit_transform(y_train)\n",
    "y_train = to_categorical(encoded)\n",
    "y_test = to_categorical(encoder.transform(y_test))\n",
    "\n",
    "print(\"Author {} is one-hot encoded as:\\n\".format(df[\"author\"][0]), y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network output shape (# of trained authors): 41\n"
     ]
    }
   ],
   "source": [
    "output_shape = len(y_train[0])\n",
    "print(\"Network output shape (# of trained authors):\", output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X \n",
    "del y \n",
    "del embed_model \n",
    "del df\n",
    "del X_chunks\n",
    "del y_chunks\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "    \n",
    "model = Sequential()\n",
    "#model.add(LSTM(256, dropout=0.3, input_shape=(WINDOW_SIZE, embed_size), return_sequences=False))\n",
    "model.add(LSTM(256, dropout=0.5, recurrent_dropout=0.2, input_shape=(WINDOW_SIZE, embed_size), return_sequences=True))\n",
    "model.add(LSTM(256, dropout=0.2))\n",
    "model.add(Dense(output_shape, activation='softmax', name='output'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18195 samples, validate on 4549 samples\n",
      "Epoch 1/50\n",
      "18195/18195 [==============================] - 53s 3ms/step - loss: 3.6242 - acc: 0.0499 - val_loss: 3.3537 - val_acc: 0.0842\n",
      "Epoch 2/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 3.3630 - acc: 0.0901 - val_loss: 3.0812 - val_acc: 0.1438\n",
      "Epoch 3/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 3.2269 - acc: 0.1084 - val_loss: 3.0587 - val_acc: 0.1416\n",
      "Epoch 4/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 3.1675 - acc: 0.1196 - val_loss: 2.9433 - val_acc: 0.1570\n",
      "Epoch 5/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 3.3469 - acc: 0.1040 - val_loss: 3.2160 - val_acc: 0.1139\n",
      "Epoch 6/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 3.2064 - acc: 0.1250 - val_loss: 2.9284 - val_acc: 0.1759\n",
      "Epoch 7/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 3.0397 - acc: 0.1514 - val_loss: 2.7990 - val_acc: 0.2071\n",
      "Epoch 8/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.9430 - acc: 0.1625 - val_loss: 2.6859 - val_acc: 0.2266\n",
      "Epoch 9/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.9027 - acc: 0.1725 - val_loss: 2.6672 - val_acc: 0.2354\n",
      "Epoch 10/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.8224 - acc: 0.1925 - val_loss: 2.5597 - val_acc: 0.2519\n",
      "Epoch 11/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.7691 - acc: 0.2014 - val_loss: 2.5521 - val_acc: 0.2543\n",
      "Epoch 12/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.7270 - acc: 0.2077 - val_loss: 2.4899 - val_acc: 0.2667\n",
      "Epoch 13/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.6827 - acc: 0.2207 - val_loss: 2.4167 - val_acc: 0.2990\n",
      "Epoch 14/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.6619 - acc: 0.2247 - val_loss: 2.4672 - val_acc: 0.2752\n",
      "Epoch 15/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.6310 - acc: 0.2331 - val_loss: 2.3828 - val_acc: 0.2917\n",
      "Epoch 16/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.5838 - acc: 0.2435 - val_loss: 2.3141 - val_acc: 0.3093\n",
      "Epoch 17/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.5507 - acc: 0.2523 - val_loss: 2.2902 - val_acc: 0.3203\n",
      "Epoch 18/50\n",
      "18195/18195 [==============================] - 50s 3ms/step - loss: 2.5287 - acc: 0.2552 - val_loss: 2.2218 - val_acc: 0.3410\n",
      "Epoch 19/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.4853 - acc: 0.2663 - val_loss: 2.2017 - val_acc: 0.3458\n",
      "Epoch 20/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.4612 - acc: 0.2714 - val_loss: 2.1760 - val_acc: 0.3504\n",
      "Epoch 21/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.4117 - acc: 0.2807 - val_loss: 2.1631 - val_acc: 0.3506\n",
      "Epoch 22/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.3885 - acc: 0.2887 - val_loss: 2.0970 - val_acc: 0.3687\n",
      "Epoch 23/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.3579 - acc: 0.2979 - val_loss: 2.0701 - val_acc: 0.3682\n",
      "Epoch 24/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.3248 - acc: 0.3044 - val_loss: 2.0425 - val_acc: 0.3834\n",
      "Epoch 25/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.2835 - acc: 0.3110 - val_loss: 1.9878 - val_acc: 0.3924\n",
      "Epoch 26/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.2603 - acc: 0.3194 - val_loss: 1.9555 - val_acc: 0.4098\n",
      "Epoch 27/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.2451 - acc: 0.3210 - val_loss: 1.9417 - val_acc: 0.4098\n",
      "Epoch 28/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.2004 - acc: 0.3349 - val_loss: 1.9216 - val_acc: 0.4133\n",
      "Epoch 29/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.1572 - acc: 0.3511 - val_loss: 1.8685 - val_acc: 0.4291\n",
      "Epoch 30/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.1444 - acc: 0.3489 - val_loss: 1.8568 - val_acc: 0.4271\n",
      "Epoch 31/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.1054 - acc: 0.3541 - val_loss: 1.8504 - val_acc: 0.4293\n",
      "Epoch 32/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.0659 - acc: 0.3668 - val_loss: 1.7780 - val_acc: 0.4471\n",
      "Epoch 33/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.0483 - acc: 0.3770 - val_loss: 1.7677 - val_acc: 0.4520\n",
      "Epoch 34/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 2.0130 - acc: 0.3830 - val_loss: 1.7558 - val_acc: 0.4432\n",
      "Epoch 35/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 1.9959 - acc: 0.3869 - val_loss: 1.7053 - val_acc: 0.4700\n",
      "Epoch 36/50\n",
      "18195/18195 [==============================] - 49s 3ms/step - loss: 1.9637 - acc: 0.3985 - val_loss: 1.6807 - val_acc: 0.4790\n",
      "Epoch 37/50\n",
      "15360/18195 [========================>.....] - ETA: 6s - loss: 1.9333 - acc: 0.4081"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          batch_size=512,\n",
    "          epochs=50,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3323/3323 [==============================] - 18s 6ms/step\n",
      "Test score: 0.8724555654443964\n",
      "Test accuracy: 0.7123081552634353\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"data/5-100-lstm256-256-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
