{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
    "import tensorflow.contrib.rnn as rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read in dataset\n",
    "Load the [Reuter 50_50 training dataset](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50).\n",
    "\n",
    "TODO:  download and extract directly from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>China on Tuesday announced a ban on poultry an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>China said on Thursday the highest-level U.S. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>China has tightened safety measures after a fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>China on Thursday tried to play down friction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>China is preparing to tap overseas capital mar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                               text\n",
       "0  WilliamKazer  China on Tuesday announced a ban on poultry an...\n",
       "1  WilliamKazer  China said on Thursday the highest-level U.S. ...\n",
       "2  WilliamKazer  China has tightened safety measures after a fa...\n",
       "3  WilliamKazer  China on Thursday tried to play down friction ...\n",
       "4  WilliamKazer  China is preparing to tap overseas capital mar..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source modified from:\n",
    "# https://github.com/devanshdalal/Author-Identification-task/blob/master/learner.py\n",
    "path = 'data/C50/C50train/'\n",
    "authors = os.listdir(path)\n",
    "data = []\n",
    "\n",
    "for author in authors:\n",
    "  texts = os.listdir(path + author + '/')\n",
    "  for text in texts:\n",
    "    f=open(path + author + '/' + text, 'r')\n",
    "    data.append([author, f.read()])\n",
    "    f.close()\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"author\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "# download 'punkt' if this is first time in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>china on tuesday announced a ban on poultry an...</td>\n",
       "      <td>[china, on, tuesday, announced, a, ban, on, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>china said on thursday the highest-level u.s. ...</td>\n",
       "      <td>[china, said, on, thursday, the, highest-level...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>china has tightened safety measures after a fa...</td>\n",
       "      <td>[china, has, tightened, safety, measures, afte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>china on thursday tried to play down friction ...</td>\n",
       "      <td>[china, on, thursday, tried, to, play, down, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>china is preparing to tap overseas capital mar...</td>\n",
       "      <td>[china, is, preparing, to, tap, overseas, capi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                               text  \\\n",
       "0  WilliamKazer  china on tuesday announced a ban on poultry an...   \n",
       "1  WilliamKazer  china said on thursday the highest-level u.s. ...   \n",
       "2  WilliamKazer  china has tightened safety measures after a fa...   \n",
       "3  WilliamKazer  china on thursday tried to play down friction ...   \n",
       "4  WilliamKazer  china is preparing to tap overseas capital mar...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [china, on, tuesday, announced, a, ban, on, po...  \n",
       "1  [china, said, on, thursday, the, highest-level...  \n",
       "2  [china, has, tightened, safety, measures, afte...  \n",
       "3  [china, on, thursday, tried, to, play, down, f...  \n",
       "4  [china, is, preparing, to, tap, overseas, capi...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "df[\"text\"] = df[\"text\"].str.replace('\\n', ' ')\n",
    "df[\"tokens\"] = df[\"text\"].apply(nltk.word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gather vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters dataset\n",
      "Total words: 1435269\n",
      "Unique words: 37279\n"
     ]
    }
   ],
   "source": [
    "all_text = df[\"text\"].str.cat()\n",
    "all_text = nltk.word_tokenize(all_text)\n",
    "\n",
    "print(\"Reuters dataset\")\n",
    "print(\"Total words: {}\".format(len(all_text)))\n",
    "print(\"Unique words: {}\".format(len(set(all_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Reuters words:\n",
      " ['the', ',', '.', 'to', 'of', 'a', 'in', 'and', 'said', \"'s\", '``', \"''\", 'for', 'on', 'that', 'is', 'it', 'with', 'be', '$', 'at', 'by', 'its', 'as', 'was', 'from', 'he', 'will', 'but', 'has', 'have', 'would', 'percent', 'are', 'million', 'not', 'which', 'an', 'year', '(', ')', 'this', 'we', 'company', 'had', 'new', 'they', 'market', 'were', 'china', 'billion', 'up', 'been', 'more', 'one', '--', 'also', 'or', 'about', 'analysts', 'after', 'u.s.', 'last', 'their', 'than', 'some', 'over', 'there', 'could', 'who', 'two', 'group', 'share', 'first', 'i', 'companies', 'hong', 'industry', 'business', 'kong', 'other', 'his', 'if', 'bank', 'into', 'stock', 'government', 'expected', 'years', 'out', 'shares', 'analyst', 'sales', 'no', ';', 'all', 'told', 'when', 'chinese', 'next']\n"
     ]
    }
   ],
   "source": [
    "# Create a vocab of the Reuters dataset, ordered by frequency (decending)\n",
    "f = nltk.FreqDist(all_text)\n",
    "reuters_most_common = [w for (w,_) in f.most_common()]\n",
    "print(\"Top 100 Reuters words:\\n\", reuters_most_common[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Google words:\n",
      " ['the', 'of', 'and', 'to', 'a', 'in', 'for', 'is', 'on', 'that', 'by', 'this', 'with', 'i', 'you', 'it', 'not', 'or', 'be', 'are', 'from', 'at', 'as', 'your', 'all', 'have', 'new', 'more', 'an', 'was', 'we', 'will', 'home', 'can', 'us', 'about', 'if', 'page', 'my', 'has', 'search', 'free', 'but', 'our', 'one', 'other', 'do', 'no', 'information', 'time', 'they', 'site', 'he', 'up', 'may', 'what', 'which', 'their', 'news', 'out', 'use', 'any', 'there', 'see', 'only', 'so', 'his', 'when', 'contact', 'here', 'business', 'who', 'web', 'also', 'now', 'help', 'get', 'pm', 'view', 'online', 'c', 'e', 'first', 'am', 'been', 'would', 'how', 'were', 'me', 's', 'services', 'some', 'these', 'click', 'its', 'like', 'service', 'x', 'than', 'find']\n"
     ]
    }
   ],
   "source": [
    "# Load top 20,000 English words, according to Google\n",
    "#    Source:  https://github.com/first20hours/google-10000-english\n",
    "with open('data/google-10000-english-usa-no-swears.txt', 'r') as f:\n",
    "  google_most_common = f.read().replace('\\n', ' ')\n",
    "google_most_common = nltk.word_tokenize(google_most_common)\n",
    "\n",
    "print(\"Top 100 Google words:\\n\", google_most_common[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters unique words: 12974. Here's the top 100.\n",
      "[',', '.', \"'s\", '``', \"''\", '$', '(', ')', '--', 'u.s.', ';', '&', \"n't\", 'corp.', \"'\", '1996', '10', '...', '1997', 'inc.', 'tonnes', '1995', 'pence', '20', '30', 'wang', \"'re\", 'mci', '1', ':', 'newsroom', 'boeing', '15', '50', 'bre-x', '100', 'airbus', 'co.', 'tung', 'francs', 'takeover', '12', 'traders', '40', '25', 'rival', \"'ve\", 'uaw', 'klaus', 'stg', '14', 'cocoa', 'yuan', 'barrick', 'shareholder', '-', 'labour', '11', '1997.', 'ltd.', 'conrail', '1996/97', '60', '1995.', 'speculation', 'margins', '300', 'regulators', 'long-term', 'automaker', 'tibet', '1998', 'long-distance', 'murdoch', '2', '1994', '90', 'exporters', 'jiang', 'handover', 'telecoms', '1989', '?', 'crowns', 'privatisation', 'eurotunnel', '17', '16', '171', 'rivals', 'jumped', 'adm', 'dissident', 'csx', 'deng', 'profitable', '80', '200', '18', '13']\n",
      "\n",
      "Google unique words: 2861. Here's the top 100.\n",
      "['pm', 'info', 'ebay', 'k', 'y', 'teen', 'gay', 'forums', 'blog', 'login', 'quote', 'girls', 'z', 'poker', 'rss', 'garden', 'color', 'archive', 'faq', 'mar', 'url', 'downloads', 'apr', 'edit', 'linux', 'jul', 'archives', 'jun', 'google', 'html', 'oh', 'pics', 'ny', 'eur', 'pdf', 'usr', 'dc', 'mon', 'homepage', 'pre', 'zip', 'lesbian', 'ms', 'fri', 'courses', 'hi', 'wed', 'male', 'inn', 'cnet', 'featured', 'eg', 'ip', 'tue', 'thank', 'pa', 'paypal', 'thu', 'anti', 'nokia', 'tel', 'overview', 'accommodation', 'tx', 'ie', 'guides', 'amazon', 'animals', 'xml', 'fl', 'fitness', 'mb', 'abstract', 'multi', 'photography', 'prev', 'il', 'filter', 'int', 'mini', 'disclaimer', 'faculty', 'usb', 'php', 'phentermine', 'keywords', 'reader', 'isbn', 'az', 'un', 'pst', 'mi', 'sub', 'careers', 'blogs', 'galleries', 'literature', 'musical', 'kb', 'bible']\n"
     ]
    }
   ],
   "source": [
    "# Look through the differences between the two vocabs\n",
    "unique_reuters_words = [x for x in reuters_most_common[:20000] if x not in google_most_common]\n",
    "print(\"Reuters unique words: {}. Here's the top 100.\".format(len(unique_reuters_words)))\n",
    "print(unique_reuters_words[:100])\n",
    "\n",
    "unique_google_words = [x for x in google_most_common if x not in reuters_most_common[:20000]]\n",
    "print(\"\\nGoogle unique words: {}. Here's the top 100.\".format(len(unique_google_words)))\n",
    "print(unique_google_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's take a look at some of these unique words, in order of frequency, to see if they are domain-specific.  For the Reuters vocab, some are punctuation, many are numbers, and the remainder are mostly domain-specific (international-business) related words, such as *privitisation, pre-tax*, and *conglomerate* or names such as *murdoch* and *monsanto*.  For the Google vocab, some are letters, some computer-related such as *forums* and *login*, and some are more general such as *color* and *thank*.\n",
    "\n",
    "Let's use the Google vocab and add punctuations and contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "# Extend common vocab to include punctuation + contractions\n",
    "from string import punctuation\n",
    "vocab = most_common + list(punctuation) + ['--', \"'s\", \"n't\", '...', \"'re\", \"'ve\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Convert text and authors to integers\n",
    ".\n",
    "\n",
    "### Mapping authors to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>encoded</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>china on tuesday announced a ban on poultry an...</td>\n",
       "      <td>[china, on, tuesday, announced, a, ban, on, po...</td>\n",
       "      <td>[630, 10, 1150, 2773, 6, 5431, 10, 9878, 4, 98...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>china said on thursday the highest-level u.s. ...</td>\n",
       "      <td>[china, said, on, thursday, the, highest-level...</td>\n",
       "      <td>[630, 187, 10, 1184, 2, 1, 1, 400, 7, 117, 173...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>china has tightened safety measures after a fa...</td>\n",
       "      <td>[china, has, tightened, safety, measures, afte...</td>\n",
       "      <td>[630, 41, 1, 715, 2030, 151, 6, 8196, 7680, 90...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>china on thursday tried to play down friction ...</td>\n",
       "      <td>[china, on, thursday, tried, to, play, down, f...</td>\n",
       "      <td>[630, 10, 1184, 2353, 5, 517, 312, 13825, 14, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>china is preparing to tap overseas capital mar...</td>\n",
       "      <td>[china, is, preparing, to, tap, overseas, capi...</td>\n",
       "      <td>[630, 9, 5228, 5, 6831, 4625, 1164, 1905, 12, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                               text  \\\n",
       "0  WilliamKazer  china on tuesday announced a ban on poultry an...   \n",
       "1  WilliamKazer  china said on thursday the highest-level u.s. ...   \n",
       "2  WilliamKazer  china has tightened safety measures after a fa...   \n",
       "3  WilliamKazer  china on thursday tried to play down friction ...   \n",
       "4  WilliamKazer  china is preparing to tap overseas capital mar...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [china, on, tuesday, announced, a, ban, on, po...   \n",
       "1  [china, said, on, thursday, the, highest-level...   \n",
       "2  [china, has, tightened, safety, measures, afte...   \n",
       "3  [china, on, thursday, tried, to, play, down, f...   \n",
       "4  [china, is, preparing, to, tap, overseas, capi...   \n",
       "\n",
       "                                             encoded  labels  \n",
       "0  [630, 10, 1150, 2773, 6, 5431, 10, 9878, 4, 98...       1  \n",
       "1  [630, 187, 10, 1184, 2, 1, 1, 400, 7, 117, 173...       1  \n",
       "2  [630, 41, 1, 715, 2030, 151, 6, 8196, 7680, 90...       1  \n",
       "3  [630, 10, 1184, 2353, 5, 517, 312, 13825, 14, ...       1  \n",
       "4  [630, 9, 5228, 5, 6831, 4625, 1164, 1905, 12, ...       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode authors and labels\n",
    "author_to_int = {c: i for i, c in enumerate(authors, 1)}\n",
    "df[\"labels\"] = [author_to_int[name] for name in df[\"author\"].values]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# https://github.com/RaRe-Technologies/gensim-data\n",
    "info = api.info()\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.wv['woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create sequences and batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sample and split dataframe:  60% training, 20% validation, and 20% test\n",
    "train = df.sample(frac=0.6, replace=False, random_state=1)\n",
    "test = df.drop(train.index)\n",
    "val = test.sample(frac=0.5, replace=False, random_state=1)\n",
    "test = test.drop(val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(1500, 50) \n",
      "Validation set: \t(500, 50) \n",
      "Test set: \t\t(500, 50)\n"
     ]
    }
   ],
   "source": [
    "def get_values(dataframe):\n",
    "  x = [model.wv(x) for x in dataframe[\"tokens\"].values]\n",
    "  y = [author_to_int(y) for y in dataframe[\"authors\"].values]\n",
    "  return np.array(x), np.array(y)\n",
    "\n",
    "train_x, train_y = get_values(train)\n",
    "val_x, val_y = get_values(val)\n",
    "test_x, test_y = get_values(test)\n",
    "\n",
    "# Source below from:  https://github.com/udacity/deep-learning/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
