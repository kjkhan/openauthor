{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install keras gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in dataset\n",
    "Load the [Reuter 50_50 training dataset](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50).\n",
    "\n",
    "TODO:  download and extract directly from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Drugstore giant Revco D.S. Inc. said Monday it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Mattel Inc., seeking to expand in the market f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>A financial agreement between Barney's Inc and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>An independent shareholder advisory firm recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Raising the stakes in the escalating battle fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text\n",
       "0  RobinSidel  Drugstore giant Revco D.S. Inc. said Monday it...\n",
       "1  RobinSidel  Mattel Inc., seeking to expand in the market f...\n",
       "2  RobinSidel  A financial agreement between Barney's Inc and...\n",
       "3  RobinSidel  An independent shareholder advisory firm recom...\n",
       "4  RobinSidel  Raising the stakes in the escalating battle fo..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source modified from:\n",
    "# https://github.com/devanshdalal/Author-Identification-task/blob/master/learner.py\n",
    "path = 'data/C50/C50train/'\n",
    "authors = os.listdir(path)\n",
    "data = []\n",
    "\n",
    "for author in authors:\n",
    "  texts = os.listdir(path + author + '/')\n",
    "  for text in texts:\n",
    "    f=open(path + author + '/' + text, 'r')\n",
    "    data.append([author, f.read()])\n",
    "    f.close()\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"author\", \"text\"])\n",
    "df.head()\n",
    "\n",
    "# TODO: add more author, text pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "codeCollapsed": false,
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "# download 'punkt' if this is first time in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 tokens in text:\n",
      " ['drugstore', 'giant', 'revco', 'd.s', '.', 'inc.', 'said', 'monday', 'it', 'agreed', 'to', 'buy', 'regional', 'chain', 'big', 'b', 'inc.', 'in', 'a', 'sweetened', 'takeover', 'valued', 'at', '$', '380', 'million', '.', 'the', 'transaction', 'calls', 'for', 'twinsburg', ',', 'ohio-based', 'revco', 'to', 'buy', 'all', 'outstanding', 'shares', 'of', 'big', 'b', 'common', 'stock', 'for', '$', '17.25', 'per', 'share', ',', 'up', 'from', 'revco', \"'s\", 'unsolicited', 'offer', 'of', '$', '15', 'per', 'share', ',', 'which', 'big', 'b', 'rejected', 'last', 'month', '.', '``', 'we', 'are', 'very', 'excited', 'about', 'the', 'combination', 'of', 'revco', 'and', 'big', 'b.', 'i', 'am', 'pleased', 'we', 'were', 'able', 'to', 'bring', 'this', 'process', 'to', 'a', 'fast', 'and', 'successful', 'conclusion', ',']\n"
     ]
    }
   ],
   "source": [
    "# change text to lower case, replace new lines, and tokenize\n",
    "X = df[\"text\"].str.lower().replace('\\n', ' ')\n",
    "X = [nltk.word_tokenize(x) for x in X]\n",
    "print(\"First 100 tokens in text:\\n\", X[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters dataset vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters dataset\n",
      "Total words: 1435370\n",
      "Unique words: 37268\n"
     ]
    }
   ],
   "source": [
    "all_text = [y for x in X for y in x]\n",
    "print(\"Reuters dataset\")\n",
    "print(\"Total words: {}\".format(len(all_text)))\n",
    "print(\"Unique words: {}\".format(len(set(all_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Reuters words:\n",
      " ['the', ',', '.', 'to', 'of', 'a', 'in', 'and', 'said', \"'s\", \"''\", '``', 'for', 'on', 'that', 'is', 'it', 'with', 'be', '$', 'at', 'by', 'its', 'as', 'was', 'from', 'he', 'will', 'but', 'has', 'have', 'would', 'percent', 'are', 'million', 'not', 'which', 'an', 'year', '(', ')', 'this', 'we', 'company', 'had', 'new', 'they', 'market', 'were', 'china', 'billion', 'up', 'been', 'more', 'one', '--', 'also', 'or', 'about', 'analysts', 'after', 'u.s.', 'last', 'their', 'than', 'some', 'over', 'there', 'could', 'who', 'group', 'two', 'share', 'first', 'i', 'companies', 'hong', 'industry', 'business', 'kong', 'other', 'his', 'if', 'bank', 'into', 'stock', 'government', 'expected', 'years', 'out', 'shares', 'analyst', 'sales', 'no', ';', 'all', 'told', 'when', 'chinese', 'next']\n"
     ]
    }
   ],
   "source": [
    "# Create a vocab of the Reuters dataset, ordered by frequency (decending)\n",
    "f = nltk.FreqDist(all_text)\n",
    "reuters_most_common = [w for (w,_) in f.most_common()]\n",
    "print(\"Top 100 Reuters words:\\n\", reuters_most_common[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google's top 10,000 word vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Google words:\n",
      " ['the', 'of', 'and', 'to', 'a', 'in', 'for', 'is', 'on', 'that', 'by', 'this', 'with', 'i', 'you', 'it', 'not', 'or', 'be', 'are', 'from', 'at', 'as', 'your', 'all', 'have', 'new', 'more', 'an', 'was', 'we', 'will', 'home', 'can', 'us', 'about', 'if', 'page', 'my', 'has', 'search', 'free', 'but', 'our', 'one', 'other', 'do', 'no', 'information', 'time', 'they', 'site', 'he', 'up', 'may', 'what', 'which', 'their', 'news', 'out', 'use', 'any', 'there', 'see', 'only', 'so', 'his', 'when', 'contact', 'here', 'business', 'who', 'web', 'also', 'now', 'help', 'get', 'pm', 'view', 'online', 'c', 'e', 'first', 'am', 'been', 'would', 'how', 'were', 'me', 's', 'services', 'some', 'these', 'click', 'its', 'like', 'service', 'x', 'than', 'find']\n"
     ]
    }
   ],
   "source": [
    "# Load top 10,000 English words, according to Google\n",
    "#    Source:  https://github.com/first20hours/google-10000-english\n",
    "with open('data/google-10000-english-usa-no-swears.txt', 'r') as f:\n",
    "  google_most_common = f.read().replace('\\n', ' ')\n",
    "google_most_common = nltk.word_tokenize(google_most_common)\n",
    "print(\"Top 100 Google words:\\n\", google_most_common[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare vocabs and choose one for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "codeCollapsed": false,
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters unique words: 4503. Here's the top 100.\n",
      "[',', '.', \"'s\", \"''\", '``', '$', '(', ')', '--', 'u.s.', ';', '&', \"n't\", 'corp.', \"'\", '1996', '10', '1997', '...', 'inc.', 'tonnes', '1995', 'pence', '20', '30', 'wang', \"'re\", 'mci', '1', ':', 'newsroom', 'boeing', '15', '50', 'bre-x', '100', 'airbus', 'co.', 'tung', 'francs', 'takeover', '12', 'traders', '40', '25', 'rival', \"'ve\", 'uaw', 'klaus', 'stg', '14', 'cocoa', 'yuan', 'barrick', 'shareholder', '-', 'labour', '11', 'ltd.', 'conrail', '1996/97', '60', 'speculation', 'margins', '1997.', '1995.', '300', 'regulators', 'long-term', 'automaker', '1998', 'tibet', '1994', 'long-distance', 'murdoch', '2', 'exporters', '90', 'jiang', 'handover', 'telecoms', '1989', '?', 'eurotunnel', 'crowns', 'privatisation', '16', '17', '171', 'rivals', 'jumped', 'adm', 'dissident', 'csx', 'deng', 'profitable', '80', '200', '18', '13']\n",
      "\n",
      "Google unique words: 4392. Here's the top 100.\n",
      "['pm', 'e', 'click', 'x', 're', 'info', 'm', 'ebay', 'dvd', 'website', 'photo', 'description', 'non', 'k', 'y', 'reply', 'teen', 'photos', 'gay', 'thread', 'gallery', 'library', 'accessories', 'forums', 'dec', 'cart', 'feedback', 'blog', 'login', 'quote', 'girls', 'z', 'poker', 'browse', 'files', 'rss', 'garden', 'color', 'self', 'archive', 'log', 'faq', 'fun', 'feb', 'mar', 'pro', 'url', 'aug', 'downloads', 'apr', 'tickets', 'topics', 'tips', 'edit', 'en', 'lyrics', 'linux', 'jul', 'archives', 'error', 'jun', 'girl', 'chapter', 'google', 'html', 'advertise', 'oh', 'pics', 'contents', 'album', 'ny', 'eur', 'pdf', 'usr', 'jewelry', 'dc', 'mon', 'com', 'homepage', 'pre', 'zip', 'lesbian', 'score', 'ok', 'sample', 'ms', 'fri', 'courses', 'hi', 'mode', 'button', 'wed', 'male', 'custom', 'inn', 'cnet', 'featured', 'female', 'eg', 'ip']\n"
     ]
    }
   ],
   "source": [
    "# Look through the differences between the two vocabs\n",
    "unique_reuters_words = [x for x in reuters_most_common[:10000] if x not in google_most_common]\n",
    "print(\"Reuters unique words: {}. Here's the top 100.\".format(len(unique_reuters_words)))\n",
    "print(unique_reuters_words[:100])\n",
    "\n",
    "unique_google_words = [x for x in google_most_common if x not in reuters_most_common[:10000]]\n",
    "print(\"\\nGoogle unique words: {}. Here's the top 100.\".format(len(unique_google_words)))\n",
    "print(unique_google_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of these unique words, in order of frequency, to see if they are domain-specific.  For the Reuters vocab, some are punctuation, many are numbers, and the remainder are mostly domain-specific (international-business) related words, such as *privitisation, pre-tax*, and *conglomerate* or names such as *murdoch* and *monsanto*.  For the Google vocab, some are letters, some computer-related such as *forums* and *login*, and some are more general such as *color* and *thank*.\n",
    "\n",
    "Let's use the Google vocab and add punctuations and contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "# Extend common vocab to include punctuation + contractions\n",
    "from string import punctuation\n",
    "vocab = google_most_common + list(punctuation) + ['--', \"'s\", \"n't\", '...', \"'re\", \"'ve\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text and authors to network-ready input\n",
    "### Download embedding model to represent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# https:/github.com/RaRe-Technologies/gensim-data\n",
    "# glove-twitter-25\n",
    "# word2vec-google-news-300\n",
    "info = api.info()\n",
    "embed_model = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'house':\n",
      " [-1.3345e-01  3.4688e-01  3.0748e-01 -2.1794e-03  7.1898e-01 -2.8725e-03\n",
      "  9.5989e-02  5.5276e-01  1.2153e-01 -2.6555e-01 -1.0277e+00  7.2278e-01\n",
      " -4.2767e+00 -9.0406e-02  1.1909e-01 -5.0647e-02 -3.3165e-01 -1.8213e-01\n",
      " -3.6218e-01  6.9813e-03  2.0147e-01 -2.9150e-01 -1.6417e-01 -2.8022e-01\n",
      "  5.4800e-01 -5.8081e-01  3.8146e-01 -5.5519e-01  1.6094e-01 -5.2039e-02\n",
      " -1.4798e-01  1.0892e-03 -2.6702e-01 -1.7885e-01  5.1449e-02  6.7434e-02\n",
      "  9.5654e-02  5.6137e-01  7.1208e-03  4.7000e-01 -3.1460e-01  1.0552e+00\n",
      "  5.2215e-01 -4.8432e-01  2.8615e-01  7.9474e-02  6.4211e-01  6.5274e-01\n",
      " -2.6493e-01 -8.9566e-02 -2.6298e-01 -3.4906e-01  3.3645e-02  2.1278e-01\n",
      " -1.0738e+00 -3.6867e-01  1.8473e-01  3.3821e-01  5.7516e-01  1.7559e-01\n",
      " -1.5436e-01  5.2836e-02 -9.8523e-02 -4.0975e-01 -8.5839e-02 -3.1527e-01\n",
      "  1.7936e-01 -2.0953e-01  6.6424e-01 -5.7412e-02  2.4528e-01 -2.2577e-01\n",
      " -3.3233e-01  2.1225e-01  2.3743e-01  1.3298e-01 -4.4889e-01  4.9577e-01\n",
      "  4.3360e-01  2.4248e-01  1.6624e+00  4.2981e-01 -4.8961e-01 -2.3809e-01\n",
      "  1.6583e-01 -4.9037e-01  3.6121e-01  8.0868e-01  5.0630e-01 -6.9646e-02\n",
      " -5.2503e-01 -7.9513e-03  5.3885e-01 -7.6658e-02 -2.5745e-01  6.0910e-01\n",
      "  4.5299e-01 -3.2974e-01 -5.1177e-01 -2.7013e-01]\n",
      "\n",
      "Similar words to 'house':\n",
      " [('room', 0.8465880155563354), ('home', 0.8294692635536194), ('party', 0.7633090019226074), ('going', 0.7493972778320312), ('out', 0.7491909861564636), ('office', 0.747423529624939), ('now', 0.7471632957458496), ('apartment', 0.7466973662376404), ('up', 0.7463924884796143), ('family', 0.743867039680481)]\n"
     ]
    }
   ],
   "source": [
    "# print sample data\n",
    "print(\"Embedding for 'house':\\n\", embed_model.wv['house'])\n",
    "print(\"\\nSimilar words to 'house':\\n\", embed_model.most_similar(\"house\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer representation of first text:\n",
      " [0, 4096, 0, 0, 9904, 0, 185, 1009, 16, 2751, 4, 130, 1033, 2222, 329, 124, 0, 6, 5, 0, 0, 8494, 22, 9894, 0, 662, 9904, 1, 3411, 1868, 7, 0, 9902, 0, 0, 4, 130, 25, 3459, 2807, 2, 329, 124, 733, 433, 7, 9894, 0, 322, 658, 9902, 54, 21, 0, 9924, 0, 616, 2, 9894, 0, 322, 658, 9902, 57, 329, 124, 6605, 126, 636, 9904, 0, 31, 20, 174, 6285, 36, 1, 2787, 2, 0, 3, 329, 0, 14, 84, 4956, 31, 88, 724, 4, 1418, 12, 430, 4, 5, 848, 3, 1885, 3994, 9902, 0, 185, 0, 0, 9902, 686, 3, 1781, 1098, 1687, 2, 0, 9904, 1, 1270, 32, 4982, 1, 1837, 9924, 0, 3, 0, 1014, 218, 6151, 9904, 1, 665, 185, 329, 124, 9924, 344, 2, 2774, 0, 1574, 1, 587, 616, 3, 1567, 10, 329, 124, 7339, 6665, 58, 2807, 9904, 1, 424, 2, 0, 9902, 0, 329, 124, 3687, 1, 587, 3759, 6, 1, 5367, 0, 0, 478, 9904, 0, 1642, 0, 2088, 29, 705, 12, 636, 4, 130, 443, 1714, 2222, 0, 0, 0, 7, 9894, 0, 2501, 9902, 3, 0, 1014, 0, 9902, 5, 851, 2, 0, 0, 0, 0, 9902, 2751, 6, 559, 4, 130, 0, 9924, 0, 7, 36, 9894, 0, 662, 9904, 1013, 329, 124, 6605, 1, 1956, 616, 9902, 95, 433, 1311, 106, 85, 1866, 22, 36, 9894, 0, 322, 658, 9, 1040, 10, 1, 178, 86, 19, 1886, 7, 28, 99, 0, 9924, 688, 9894, 0, 322, 658, 616, 9904, 0, 9924, 433, 0, 54, 0, 5973, 22, 9894, 0, 5, 658, 9, 1, 27, 411, 433, 1046, 9904, 329, 124, 9924, 2807, 88, 22, 9894, 0, 9902, 54, 0, 5973, 9902, 6, 3852, 1866, 9, 7552, 9904, 0, 13, 0, 9924, 525, 326, 9902, 6399, 3973, 3, 719, 3, 444, 4886, 9902, 31, 34, 845, 2898, 44, 2645, 178, 9924, 403, 984, 3, 981, 44, 444, 1220, 9904, 962, 46, 0, 9902, 12, 2787, 32, 1198, 1, 2645, 178, 4, 3098, 788, 110, 5, 1325, 984, 2, 549, 9902, 0, 0, 185, 9904, 329, 124, 185, 16, 32, 125, 13, 0, 4, 1366, 5, 3843, 3643, 9904, 96, 183, 46, 1033, 0, 6151, 9902, 329, 124, 40, 85, 3642, 646, 10, 26, 7143, 1, 0, 0, 2, 2570, 117, 308, 665, 3, 1129, 971, 6249, 9904, 16, 40, 85, 5037, 11, 0, 2294, 0, 3, 444, 3, 95, 3741, 26, 9596, 1368, 522, 9904, 478, 2475, 495, 0, 8, 0, 4, 3963, 5060, 6, 1, 5181, 11, 1141, 2398, 2, 329, 124, 9924, 0, 1215, 821, 3, 3995, 0, 549, 6, 1707, 9904, 0, 0, 32, 19, 724, 4, 7507, 329, 124, 471, 2003, 9902, 0, 185, 2889, 0, 2, 7488, 0, 181, 9904, 329, 124, 9924, 2294, 0, 2095, 4, 0, 126, 112, 758, 4, 5253, 6, 0, 1, 1331, 2, 1100, 2937, 273, 1349, 11, 2570, 308, 2068, 9904, 1013, 16, 0, 0, 6, 5, 27, 311, 136, 4, 494, 698, 2294, 0, 9902, 105, 4217, 13, 1, 178, 26, 185, 95, 3517, 4779, 1215, 821, 8, 1266, 22, 65, 0, 847, 2, 1676, 9904, 7617, 185, 45, 2, 0, 9924, 83, 2097, 32, 19, 4, 5146, 218, 0, 9902, 1373, 6, 1707, 9902, 159, 0, 7030, 0, 549, 3, 329, 124, 40, 0, 549, 9904, 0, 702, 51, 77, 1, 1215, 3, 1, 549, 845, 9902, 16, 32, 19, 5, 211, 2294, 821, 7, 0, 9902, 0, 185, 371, 0, 9902, 2, 1162, 3167, 1842, 0, 525, 181, 9904, 0, 74, 185, 16, 32, 0, 3, 4485, 95, 6665, 616, 7, 329, 124, 2807, 691, 615, 0, 1, 616, 106, 85, 188, 4, 0, 1009, 9904, 1, 116, 665, 74, 185, 25, 5818, 259, 133, 32, 19, 0, 9904, 150, 92, 0, 36, 228, 9902, 0, 2343, 12, 636, 2088, 5, 9395, 705, 13, 329, 124, 9902, 1211, 5, 0, 22, 95, 163, 3, 4349, 5576, 56, 16, 1684, 9904, 1, 329, 124, 344, 1924, 110, 1, 2261, 3, 1740, 1, 27, 3, 972, 616, 21, 0, 9904, 226, 2, 46, 544, 88, 17, 2121, 923, 9902, 1013, 0, 0, 30, 2237, 23, 5, 1220, 5339, 9904, 0, 9082, 4, 524, 9904, 329, 124, 9902, 0, 7030, 0, 1446, 1610, 1, 0, 209, 251, 9904, 0, 7030, 0, 549, 6, 0, 0, 0, 9902, 0, 3, 2082, 251, 9904, 1, 1270, 13, 329, 124, 30, 1028, 23, 5, 1399, 3759, 7, 0, 9902, 57, 2343, 12, 112, 30, 1370, 6, 5, 2572, 3411, 13, 0, 1642, 9904, 1, 116, 665, 106, 2088, 5, 0, 7, 0, 1642, 4, 130, 0, 7, 9894, 0, 2501, 9902, 43, 1, 1270, 0, 150, 0, 3061, 3088, 10, 1, 1102, 86, 3518, 1181, 273, 9904]\n"
     ]
    }
   ],
   "source": [
    "# convert text to vectors based on frequency\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab, 1)}\n",
    "for i, tokens in enumerate(X):\n",
    "    X[i] = [vocab_to_int[x] if x in vocab else 0 for x in tokens]\n",
    "\n",
    "print(\"Integer representation of first text:\\n\", X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert integers to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "codeCollapsed": false,
    "hiddenCell": true,
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix for first word in the first text\n",
      " [ 9.5152e-02  3.7024e-01  5.4291e-01  1.9621e-01  4.8205e-02  3.2033e-01\n",
      " -5.9638e-01  1.5868e-02 -1.2989e-01 -6.3028e-01  8.1944e-02  2.4164e-01\n",
      " -6.0990e+00 -6.8557e-01  5.0354e-01 -3.4089e-02  1.1705e-01 -7.7403e-03\n",
      " -8.6512e-02  4.3617e-01 -4.3982e-01  2.6125e-01 -4.0348e-02 -1.9194e-01\n",
      "  8.3204e-02 -5.8246e-01 -3.1923e-02  1.2630e-01  4.0120e-01  6.8906e-02\n",
      " -1.0517e-01 -2.0804e-01 -4.2554e-01  4.7799e-01  3.4651e-01  2.4057e-01\n",
      "  5.0244e-02 -7.2587e-02 -2.4347e-03 -5.0342e-01 -1.0601e+00 -3.1586e-01\n",
      " -3.2457e-02 -7.6317e-02  7.9045e-01  8.6367e-02 -1.9632e-01  5.7566e-02\n",
      "  8.4129e-01 -4.2020e-01 -1.1335e-03 -8.5632e-02  6.1910e-02  2.1423e-01\n",
      " -1.0356e-01 -3.6946e-02 -2.6005e-01 -3.5657e-01  5.4321e-02  3.0875e-02\n",
      "  1.4092e-01 -9.1998e-02 -4.1841e-01 -3.1135e-01 -1.4937e-01 -2.2699e-04\n",
      " -3.3454e-01 -1.4848e-01 -1.1944e-01 -2.7174e-01  3.1320e-01 -1.0998e-01\n",
      " -4.7524e-01  1.4056e-01  3.9641e-01 -4.9413e-02 -4.2601e-01 -2.3576e-01\n",
      "  6.1482e-02 -3.5313e-02  2.4161e+00  2.8979e-01  3.8882e-01  3.6779e-01\n",
      "  2.0685e-01  1.3992e-01 -4.2459e-01  4.4590e-01  2.6234e-01 -4.4834e-01\n",
      "  3.7196e-03 -2.2521e-01  1.4764e-01 -3.6417e-01 -1.8493e-01  2.2282e-01\n",
      "  4.7626e-01 -5.1083e-01  4.6877e-01  3.4882e-01]\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrix covering the dataset vocab\n",
    "embed_vector_size = embed_model.vector_size\n",
    "embedding_matrix = \\\n",
    "  [embed_model.wv[word] if word in embed_model.vocab and word in vocab else np.zeros(embed_vector_size) \\\n",
    "            for word in reuters_most_common]\n",
    "\n",
    "print(\"Embedding matrix for first word in the first text\\n\", embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode labels (authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author RobinSidel is encoded as:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Author KouroshKarimkhany is encoded as:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# enumerate authors and create one-hot encodings\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df[\"author\"].values)\n",
    "encoded_y = encoder.transform(df[\"author\"].values)\n",
    "y = np_utils.to_categorical(encoded_y)\n",
    "\n",
    "print(\"Author {} is encoded as:\\n\".format(df[\"author\"][0]), y[0])\n",
    "print(\"Author {} is encoded as:\\n\".format(df[\"author\"][100]), y[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping text to padded sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "# Sample and split dataframe: 60% training, 20% validation, and 20% test\n",
    "train = df.sample(frac=0.6, replace=False, random_state=1)\n",
    "test = df.drop(train.index)\n",
    "val = test.sample(frac=0.5, replace=False, random_state=1)\n",
    "test = test.drop(val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "def get_values(dataframe):\n",
    "    x = [x for x in dataframe[\"tokens\"]]\n",
    "    y = [y for y in dataframe[\"author\"]]\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x_train, y_train = get_values(train)\n",
    "x_val, y_val = get_values(val)\n",
    "x_test, y_test = get_values(test)\n",
    "\n",
    "# Source below from:  https://github.com/udacity/deep-learning/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(x_train.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(x_val.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into 60% training, 20% validation, and 20% test sets\n",
    "# Testing set has some authors not seen in training\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=False)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.75, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "\n",
    "max_features = 20000\n",
    "timesteps = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(512, dropout=0.2))\n",
    "model.add(Dense(50, activation='softmax', name='output'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "          batch_size=100, \n",
    "          epochs=10,\n",
    "          validation_data=(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(x_test, y_test, batch_size=200)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
