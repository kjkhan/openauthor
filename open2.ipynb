{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in dataset\n",
    "Load the [Reuter 50_50 training dataset](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50).\n",
    "\n",
    "TODO:  download and extract directly from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Drugstore giant Revco D.S. Inc. said Monday it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Mattel Inc., seeking to expand in the market f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>A financial agreement between Barney's Inc and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>An independent shareholder advisory firm recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Raising the stakes in the escalating battle fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text\n",
       "0  RobinSidel  Drugstore giant Revco D.S. Inc. said Monday it...\n",
       "1  RobinSidel  Mattel Inc., seeking to expand in the market f...\n",
       "2  RobinSidel  A financial agreement between Barney's Inc and...\n",
       "3  RobinSidel  An independent shareholder advisory firm recom...\n",
       "4  RobinSidel  Raising the stakes in the escalating battle fo..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source modified from:\n",
    "# https://github.com/devanshdalal/Author-Identification-task/blob/master/learner.py\n",
    "path = 'data/C50/C50train/'\n",
    "authors = os.listdir(path)\n",
    "data = []\n",
    "\n",
    "for author in authors:\n",
    "  texts = os.listdir(path + author + '/')\n",
    "  for text in texts:\n",
    "    f=open(path + author + '/' + text, 'r')\n",
    "    data.append([author, f.read()])\n",
    "    f.close()\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"author\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "codeCollapsed": false,
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "# download 'punkt' if this is first time in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>drugstore giant revco d.s. inc. said monday it...</td>\n",
       "      <td>[drugstore, giant, revco, d.s, ., inc., said, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>mattel inc., seeking to expand in the market f...</td>\n",
       "      <td>[mattel, inc., ,, seeking, to, expand, in, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>a financial agreement between barney's inc and...</td>\n",
       "      <td>[a, financial, agreement, between, barney, 's,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>an independent shareholder advisory firm recom...</td>\n",
       "      <td>[an, independent, shareholder, advisory, firm,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>raising the stakes in the escalating battle fo...</td>\n",
       "      <td>[raising, the, stakes, in, the, escalating, ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text  \\\n",
       "0  RobinSidel  drugstore giant revco d.s. inc. said monday it...   \n",
       "1  RobinSidel  mattel inc., seeking to expand in the market f...   \n",
       "2  RobinSidel  a financial agreement between barney's inc and...   \n",
       "3  RobinSidel  an independent shareholder advisory firm recom...   \n",
       "4  RobinSidel  raising the stakes in the escalating battle fo...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [drugstore, giant, revco, d.s, ., inc., said, ...  \n",
       "1  [mattel, inc., ,, seeking, to, expand, in, the...  \n",
       "2  [a, financial, agreement, between, barney, 's,...  \n",
       "3  [an, independent, shareholder, advisory, firm,...  \n",
       "4  [raising, the, stakes, in, the, escalating, ba...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "df[\"text\"] = df[\"text\"].str.replace('\\n', ' ')\n",
    "df[\"tokens\"] = df[\"text\"].apply(nltk.word_tokenize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters dataset\n",
      "Total words: 1435270\n",
      "Unique words: 37279\n"
     ]
    }
   ],
   "source": [
    "all_text = df[\"text\"].str.cat()\n",
    "all_text = nltk.word_tokenize(all_text)\n",
    "\n",
    "print(\"Reuters dataset\")\n",
    "print(\"Total words: {}\".format(len(all_text)))\n",
    "print(\"Unique words: {}\".format(len(set(all_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Reuters words:\n",
      " ['the', ',', '.', 'to', 'of', 'a', 'in', 'and', 'said', \"'s\", '``', \"''\", 'for', 'on', 'that', 'is', 'it', 'with', 'be', '$', 'at', 'by', 'its', 'as', 'was', 'from', 'he', 'will', 'but', 'has', 'have', 'would', 'percent', 'are', 'million', 'not', 'which', 'an', 'year', '(', ')', 'this', 'we', 'company', 'had', 'new', 'they', 'market', 'were', 'china', 'billion', 'up', 'been', 'more', 'one', '--', 'also', 'or', 'about', 'analysts', 'after', 'u.s.', 'last', 'their', 'than', 'some', 'over', 'there', 'could', 'who', 'group', 'two', 'share', 'first', 'i', 'companies', 'hong', 'industry', 'business', 'kong', 'other', 'his', 'if', 'bank', 'into', 'stock', 'government', 'expected', 'years', 'out', 'shares', 'analyst', 'sales', 'no', ';', 'all', 'told', 'when', 'chinese', 'next']\n"
     ]
    }
   ],
   "source": [
    "# Create a vocab of the Reuters dataset, ordered by frequency (decending)\n",
    "f = nltk.FreqDist(all_text)\n",
    "reuters_most_common = [w for (w,_) in f.most_common()]\n",
    "print(\"Top 100 Reuters words:\\n\", reuters_most_common[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Google words:\n",
      " ['the', 'of', 'and', 'to', 'a', 'in', 'for', 'is', 'on', 'that', 'by', 'this', 'with', 'i', 'you', 'it', 'not', 'or', 'be', 'are', 'from', 'at', 'as', 'your', 'all', 'have', 'new', 'more', 'an', 'was', 'we', 'will', 'home', 'can', 'us', 'about', 'if', 'page', 'my', 'has', 'search', 'free', 'but', 'our', 'one', 'other', 'do', 'no', 'information', 'time', 'they', 'site', 'he', 'up', 'may', 'what', 'which', 'their', 'news', 'out', 'use', 'any', 'there', 'see', 'only', 'so', 'his', 'when', 'contact', 'here', 'business', 'who', 'web', 'also', 'now', 'help', 'get', 'pm', 'view', 'online', 'c', 'e', 'first', 'am', 'been', 'would', 'how', 'were', 'me', 's', 'services', 'some', 'these', 'click', 'its', 'like', 'service', 'x', 'than', 'find']\n"
     ]
    }
   ],
   "source": [
    "# Load top 10,000 English words, according to Google\n",
    "#    Source:  https://github.com/first20hours/google-10000-english\n",
    "with open('data/google-10000-english-usa-no-swears.txt', 'r') as f:\n",
    "  google_most_common = f.read().replace('\\n', ' ')\n",
    "google_most_common = nltk.word_tokenize(google_most_common)\n",
    "print(\"Top 100 Google words:\\n\", google_most_common[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "codeCollapsed": false,
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters unique words: 4503. Here's the top 100.\n",
      "[',', '.', \"'s\", '``', \"''\", '$', '(', ')', '--', 'u.s.', ';', '&', \"n't\", 'corp.', \"'\", '1996', '10', '...', '1997', 'inc.', 'tonnes', '1995', 'pence', '20', '30', 'wang', \"'re\", 'mci', '1', ':', 'newsroom', 'boeing', '15', '50', 'bre-x', '100', 'airbus', 'co.', 'tung', 'francs', 'takeover', '12', 'traders', '40', '25', 'rival', \"'ve\", 'uaw', 'klaus', 'stg', '14', 'cocoa', 'yuan', 'barrick', 'shareholder', '-', 'labour', '11', '1997.', 'ltd.', 'conrail', '1996/97', '60', 'speculation', '1995.', 'margins', '300', 'regulators', 'long-term', 'automaker', 'tibet', '1998', 'long-distance', 'murdoch', '2', '1994', 'exporters', '90', 'jiang', 'handover', 'telecoms', '?', '1989', 'eurotunnel', 'crowns', 'privatisation', '16', '17', '171', 'rivals', 'jumped', 'adm', 'dissident', 'csx', 'deng', 'profitable', '80', '200', '18', '13']\n",
      "\n",
      "Google unique words: 4392. Here's the top 100.\n",
      "['pm', 'e', 'click', 'x', 're', 'info', 'm', 'ebay', 'dvd', 'website', 'photo', 'description', 'non', 'k', 'y', 'reply', 'teen', 'photos', 'gay', 'thread', 'gallery', 'library', 'accessories', 'forums', 'dec', 'cart', 'feedback', 'blog', 'login', 'quote', 'girls', 'z', 'poker', 'browse', 'files', 'rss', 'garden', 'color', 'self', 'archive', 'log', 'faq', 'fun', 'feb', 'mar', 'pro', 'url', 'aug', 'downloads', 'apr', 'tickets', 'topics', 'tips', 'edit', 'en', 'lyrics', 'linux', 'jul', 'archives', 'error', 'jun', 'girl', 'chapter', 'google', 'html', 'advertise', 'oh', 'pics', 'contents', 'album', 'ny', 'eur', 'pdf', 'usr', 'jewelry', 'dc', 'mon', 'com', 'homepage', 'pre', 'zip', 'lesbian', 'score', 'ok', 'sample', 'ms', 'fri', 'courses', 'hi', 'mode', 'button', 'wed', 'male', 'custom', 'inn', 'cnet', 'featured', 'female', 'eg', 'ip']\n"
     ]
    }
   ],
   "source": [
    "# Look through the differences between the two vocabs\n",
    "unique_reuters_words = [x for x in reuters_most_common[:10000] if x not in google_most_common]\n",
    "print(\"Reuters unique words: {}. Here's the top 100.\".format(len(unique_reuters_words)))\n",
    "print(unique_reuters_words[:100])\n",
    "\n",
    "unique_google_words = [x for x in google_most_common if x not in reuters_most_common[:10000]]\n",
    "print(\"\\nGoogle unique words: {}. Here's the top 100.\".format(len(unique_google_words)))\n",
    "print(unique_google_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of these unique words, in order of frequency, to see if they are domain-specific.  For the Reuters vocab, some are punctuation, many are numbers, and the remainder are mostly domain-specific (international-business) related words, such as *privitisation, pre-tax*, and *conglomerate* or names such as *murdoch* and *monsanto*.  For the Google vocab, some are letters, some computer-related such as *forums* and *login*, and some are more general such as *color* and *thank*.\n",
    "\n",
    "Let's use the Google vocab and add punctuations and contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "# Extend common vocab to include punctuation + contractions\n",
    "from string import punctuation\n",
    "vocab = google_most_common + list(punctuation) + ['--', \"'s\", \"n't\", '...', \"'re\", \"'ve\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text and authors to integers\n",
    "\n",
    "### Mapping authors to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>drugstore giant revco d.s. inc. said monday it...</td>\n",
       "      <td>[drugstore, giant, revco, d.s, ., inc., said, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>mattel inc., seeking to expand in the market f...</td>\n",
       "      <td>[mattel, inc., ,, seeking, to, expand, in, the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>a financial agreement between barney's inc and...</td>\n",
       "      <td>[a, financial, agreement, between, barney, 's,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>an independent shareholder advisory firm recom...</td>\n",
       "      <td>[an, independent, shareholder, advisory, firm,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>raising the stakes in the escalating battle fo...</td>\n",
       "      <td>[raising, the, stakes, in, the, escalating, ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text  \\\n",
       "0  RobinSidel  drugstore giant revco d.s. inc. said monday it...   \n",
       "1  RobinSidel  mattel inc., seeking to expand in the market f...   \n",
       "2  RobinSidel  a financial agreement between barney's inc and...   \n",
       "3  RobinSidel  an independent shareholder advisory firm recom...   \n",
       "4  RobinSidel  raising the stakes in the escalating battle fo...   \n",
       "\n",
       "                                              tokens  labels  \n",
       "0  [drugstore, giant, revco, d.s, ., inc., said, ...       1  \n",
       "1  [mattel, inc., ,, seeking, to, expand, in, the...       1  \n",
       "2  [a, financial, agreement, between, barney, 's,...       1  \n",
       "3  [an, independent, shareholder, advisory, firm,...       1  \n",
       "4  [raising, the, stakes, in, the, escalating, ba...       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode authors and labels\n",
    "author_to_int = {c: i for i, c in enumerate(authors, 1)}\n",
    "df[\"labels\"] = [author_to_int[y] for y in df[\"author\"].values]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# https:/github.com/RaRe-Technologies/gensim-data\n",
    "# glove-twitter-25\n",
    "# word2vec-google-news-300\n",
    "info = api.info()\n",
    "embed_model = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_model.wv['house'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('home', 0.9537084102630615),\n",
       " ('room', 0.9411841630935669),\n",
       " ('at', 0.9270982146263123),\n",
       " ('out', 0.9157505631446838),\n",
       " ('here', 0.9152163863182068),\n",
       " ('street', 0.9097360968589783),\n",
       " (\"'m\", 0.9017621874809265),\n",
       " ('going', 0.9013768434524536),\n",
       " ('town', 0.9007171988487244),\n",
       " ('party', 0.8996413350105286)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model.most_similar(\"house\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text tokens to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>drugstore giant revco d.s. inc. said monday it...</td>\n",
       "      <td>[drugstore, giant, revco, d.s, ., inc., said, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.98918, 0.10242, -0.53404, -0.78541, 1.101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>mattel inc., seeking to expand in the market f...</td>\n",
       "      <td>[mattel, inc., ,, seeking, to, expand, in, the...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.57181, -0.35047, 0.11173, -0.52608, -0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>a financial agreement between barney's inc and...</td>\n",
       "      <td>[a, financial, agreement, between, barney, 's,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.21294, 0.31035, 0.17694, 0.87498, 0.067926...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>an independent shareholder advisory firm recom...</td>\n",
       "      <td>[an, independent, shareholder, advisory, firm,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.54244, 0.45753, -0.45787, 0.066361, 0.5891...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>raising the stakes in the escalating battle fo...</td>\n",
       "      <td>[raising, the, stakes, in, the, escalating, ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.96966, 1.1775, -0.76991, 0.53168, 0.60541...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text  \\\n",
       "0  RobinSidel  drugstore giant revco d.s. inc. said monday it...   \n",
       "1  RobinSidel  mattel inc., seeking to expand in the market f...   \n",
       "2  RobinSidel  a financial agreement between barney's inc and...   \n",
       "3  RobinSidel  an independent shareholder advisory firm recom...   \n",
       "4  RobinSidel  raising the stakes in the escalating battle fo...   \n",
       "\n",
       "                                              tokens  labels  \\\n",
       "0  [drugstore, giant, revco, d.s, ., inc., said, ...       1   \n",
       "1  [mattel, inc., ,, seeking, to, expand, in, the...       1   \n",
       "2  [a, financial, agreement, between, barney, 's,...       1   \n",
       "3  [an, independent, shareholder, advisory, firm,...       1   \n",
       "4  [raising, the, stakes, in, the, escalating, ba...       1   \n",
       "\n",
       "                                             vectors  \n",
       "0  [[-0.98918, 0.10242, -0.53404, -0.78541, 1.101...  \n",
       "1  [[-0.57181, -0.35047, 0.11173, -0.52608, -0.12...  \n",
       "2  [[0.21294, 0.31035, 0.17694, 0.87498, 0.067926...  \n",
       "3  [[0.54244, 0.45753, -0.45787, 0.066361, 0.5891...  \n",
       "4  [[-0.96966, 1.1775, -0.76991, 0.53168, 0.60541...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_to_vector(tokens):\n",
    "    return [embed_model.wv[word] if word in embed_model.vocab else np.zeros(25) \\\n",
    "            for word in tokens]\n",
    "    \n",
    "df[\"vectors\"] = df[\"tokens\"].apply(word_to_vector)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sequences and batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and split dataframe:  60% training, 20% validation, and 20% test\n",
    "train = df.sample(frac=0.6, replace=False, random_state=1)\n",
    "test = df.drop(train.index)\n",
    "val = test.sample(frac=0.5, replace=False, random_state=1)\n",
    "test = test.drop(val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(1500,) \n",
      "Validation set: \t(500,) \n",
      "Test set: \t\t(500,)\n"
     ]
    }
   ],
   "source": [
    "def get_values(dataframe):\n",
    "    x = [x for x in dataframe[\"tokens\"]]\n",
    "    y = [y for y in dataframe[\"labels\"]]\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x_train, y_train = get_values(train)\n",
    "x_val, y_val = get_values(val)\n",
    "x_test, y_test = get_values(test)\n",
    "\n",
    "# Source below from:  https://github.com/udacity/deep-learning/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(x_train.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(x_val.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=2000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(df[\"text\"].values)\n",
    "x = tokenizer.texts_to_sequences(df[\"text\"].values)\n",
    "x = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"labels\"].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 250/1750 [===>..........................] - ETA: 2:38 - loss: -258.8098 - acc: 0.0280"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-286fa2e7704f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/oa/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/oa/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/oa/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/oa/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/oa/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "\n",
    "max_features = 30000\n",
    "timesteps = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=10, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
