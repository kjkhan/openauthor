{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model\n",
    "A second-generation model of the main \"originality score\" algorithm: preprocessing a sample paper, performing analytics, saving the document's hash, and returning a score.  Uses embedding to improve understanding."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/C50/C50all/'\n",
    "PROCESSED_DATA_PATH = 'data/inputs.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataset\n",
    "Load the [Reuter 50_50 training dataset](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50).\n",
    "\n",
    "TODO:  download and extract directly from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Drugstore giant Revco D.S. Inc. said Monday it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Mattel Inc., seeking to expand in the market f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>A financial agreement between Barney's Inc and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>ITT Corp. met with financial advisers on Thurs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>An independent shareholder advisory firm recom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text\n",
       "0  RobinSidel  Drugstore giant Revco D.S. Inc. said Monday it...\n",
       "1  RobinSidel  Mattel Inc., seeking to expand in the market f...\n",
       "2  RobinSidel  A financial agreement between Barney's Inc and...\n",
       "3  RobinSidel  ITT Corp. met with financial advisers on Thurs...\n",
       "4  RobinSidel  An independent shareholder advisory firm recom..."
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source modified from:\n",
    "# https://github.com/devanshdalal/Author-Identification-task/blob/master/learner.py\n",
    "authors = os.listdir(DATASET_PATH)\n",
    "data = []\n",
    "\n",
    "for author in authors:\n",
    "  texts = os.listdir(path + author + '/')\n",
    "  for text in texts:\n",
    "    f=open(path + author + '/' + text, 'r')\n",
    "    data.append([author, f.read()])\n",
    "    f.close()\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"author\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"author\"]\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "codeCollapsed": false,
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kahlil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download 'punkt' if this is first time in notebook\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 tokens in text:\n",
      " ['drugstore', 'giant', 'revco', 'd.s', '.', 'inc.', 'said', 'monday', 'it', 'agreed', 'to', 'buy', 'regional', 'chain', 'big', 'b', 'inc.', 'in', 'a', 'sweetened', 'takeover', 'valued', 'at', '$', '380', 'million', '.', 'the', 'transaction', 'calls', 'for', 'twinsburg', ',', 'ohio-based', 'revco', 'to', 'buy', 'all', 'outstanding', 'shares', 'of', 'big', 'b', 'common', 'stock', 'for', '$', '17.25', 'per', 'share', ',', 'up', 'from', 'revco', \"'s\", 'unsolicited', 'offer', 'of', '$', '15', 'per', 'share', ',', 'which', 'big', 'b', 'rejected', 'last', 'month', '.', '``', 'we', 'are', 'very', 'excited', 'about', 'the', 'combination', 'of', 'revco', 'and', 'big', 'b.', 'i', 'am', 'pleased', 'we', 'were', 'able', 'to', 'bring', 'this', 'process', 'to', 'a', 'fast', 'and', 'successful', 'conclusion', ',']\n"
     ]
    }
   ],
   "source": [
    "# change text to lower case, replace new lines, and tokenize\n",
    "X = df[\"text\"].str.lower().replace('\\n', ' ')\n",
    "X = [nltk.word_tokenize(x) for x in X]\n",
    "print(\"First 100 tokens in text:\\n\", X[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters dataset vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters dataset\n",
      "Total words: 2902829\n",
      "Unique words: 53207\n"
     ]
    }
   ],
   "source": [
    "all_text = [y for x in X for y in x]\n",
    "print(\"Reuters dataset\")\n",
    "print(\"Total words: {}\".format(len(all_text)))\n",
    "print(\"Unique words: {}\".format(len(set(all_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Reuters words:\n",
      " ['the', ',', '.', 'to', 'of', 'a', 'in', 'and', 'said', \"'s\", \"''\", '``', 'on', 'for', 'that', 'is', 'it', 'with', '$', 'be', 'at', 'by', 'its', 'was', 'as', 'from', 'he', 'will', 'but', 'has', 'have', 'percent', 'would', 'are', 'million', 'not', 'which', 'an', 'year', '(', ')', 'we', 'this', 'company', 'had', 'they', 'new', 'were', 'market', 'china', 'billion', 'up', 'more', 'been', '--', 'one', 'also', 'or', 'about', 'after', 'last', 'analysts', 'than', 'their', 'over', 'some', 'u.s.', 'hong', 'there', 'kong', 'could', 'who', 'two', 'i', 'group', 'business', 'share', 'first', 'other', 'his', 'government', 'companies', 'industry', 'bank', 'if', 'stock', 'expected', 'into', 'out', 'years', 'sales', 'shares', 'analyst', 'told', 'chinese', 'no', 'when', 'all', 'people', ';']\n"
     ]
    }
   ],
   "source": [
    "# Create a vocab of the Reuters dataset, ordered by frequency (decending)\n",
    "f = nltk.FreqDist(all_text)\n",
    "reuters_most_common = [w for (w,_) in f.most_common()]\n",
    "print(\"Top 100 Reuters words:\\n\", reuters_most_common[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google's top 10,000 word vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Google words:\n",
      " ['the', 'of', 'and', 'to', 'a', 'in', 'for', 'is', 'on', 'that', 'by', 'this', 'with', 'i', 'you', 'it', 'not', 'or', 'be', 'are', 'from', 'at', 'as', 'your', 'all', 'have', 'new', 'more', 'an', 'was', 'we', 'will', 'home', 'can', 'us', 'about', 'if', 'page', 'my', 'has', 'search', 'free', 'but', 'our', 'one', 'other', 'do', 'no', 'information', 'time', 'they', 'site', 'he', 'up', 'may', 'what', 'which', 'their', 'news', 'out', 'use', 'any', 'there', 'see', 'only', 'so', 'his', 'when', 'contact', 'here', 'business', 'who', 'web', 'also', 'now', 'help', 'get', 'pm', 'view', 'online', 'c', 'e', 'first', 'am', 'been', 'would', 'how', 'were', 'me', 's', 'services', 'some', 'these', 'click', 'its', 'like', 'service', 'x', 'than', 'find']\n"
     ]
    }
   ],
   "source": [
    "# Load top 10,000 English words, according to Google\n",
    "#    Source:  https://github.com/first20hours/google-10000-english\n",
    "with open('data/google-10000-english-usa-no-swears.txt', 'r') as f:\n",
    "  google_most_common = f.read().replace('\\n', ' ')\n",
    "google_most_common = nltk.word_tokenize(google_most_common)\n",
    "print(\"Top 100 Google words:\\n\", google_most_common[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare vocabs and choose one for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "codeCollapsed": false,
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters unique words: 4390. Here's the top 100.\n",
      "[',', '.', \"'s\", \"''\", '``', '$', '(', ')', '--', 'u.s.', ';', '&', \"n't\", '1996', \"'\", 'corp.', '1997', 'inc.', '...', '10', 'tonnes', '1995', '20', \"'re\", 'pence', '30', 'bre-x', '1', 'deng', '50', 'mci', ':', 'newsroom', '15', 'tung', '100', 'boeing', 'wang', 'co.', '25', 'takeover', '12', 'francs', 'traders', 'yuan', \"'ve\", '40', 'labour', 'rival', 'conrail', 'margins', 'busang', 'airbus', 'shareholder', 'ltd.', 'nomura', '14', '1995.', 'cocoa', 'stg', 'jiang', '-', '11', 'handover', '1994', 'long-term', '1996/97', '300', '2', '60', 'klaus', 'speculation', 'uaw', 'crowns', 'privatisation', '1998', '1997.', 'regulators', '90', 'barrick', 'long-distance', 'thomson-csf', 'brokerage', '1996.', 'spokeswoman', '18', '?', '200', 'tonne', 'jumped', '171', 'csx', '31', 'rivals', '16', \"'ll\", '13', 'natwest', '=', 'telecoms']\n",
      "\n",
      "Google unique words: 4279. Here's the top 100.\n",
      "['pm', 'click', 'x', 're', 'info', 'ebay', 'dvd', 'website', 'v', 'description', 'non', 'k', 'y', 'reply', 'teen', 'photos', 'gay', 'thread', 'gallery', 'library', 'accessories', 'forums', 'dec', 'cart', 'feedback', 'blog', 'login', 'q', 'quote', 'girls', 'z', 'poker', 'browse', 'nov', 'files', 'rss', 'color', 'archive', 'faq', 'fun', 'feb', 'mar', 'url', 'aug', 'downloads', 'apr', 'tips', 'edit', 'lyrics', 'linux', 'jul', 'archives', 'jun', 'girl', 'google', 'html', 'advertise', 'oh', 'pics', 'contents', 'album', 'ny', 'eur', 'pdf', 'usr', 'jewelry', 'dc', 'mon', 'homepage', 'teens', 'pre', 'zip', 'lesbian', 'logo', 'score', 'ok', 'ms', 'fri', 'courses', 'hi', 'artist', 'mode', 'button', 'wed', 'male', 'custom', 'cnet', 'featured', 'female', 'eg', 'bed', 'ip', 'maps', 'tue', 'thank', 'pa', 'paypal', 'favorite', 'thu', 'anti']\n"
     ]
    }
   ],
   "source": [
    "# Look through the differences between the two vocabs\n",
    "unique_reuters_words = [x for x in reuters_most_common[:10000] if x not in google_most_common]\n",
    "print(\"Reuters unique words: {}. Here's the top 100.\".format(len(unique_reuters_words)))\n",
    "print(unique_reuters_words[:100])\n",
    "\n",
    "unique_google_words = [x for x in google_most_common if x not in reuters_most_common[:10000]]\n",
    "print(\"\\nGoogle unique words: {}. Here's the top 100.\".format(len(unique_google_words)))\n",
    "print(unique_google_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of these unique words, in order of frequency, to see if they are domain-specific.  For the Reuters vocab, some are punctuation, many are numbers, and the remainder are mostly domain-specific (international-business) related words, such as *privitisation, pre-tax*, and *conglomerate* or names such as *murdoch* and *monsanto*.  For the Google vocab, some are letters, some computer-related such as *forums* and *login*, and some are more general such as *color* and *thank*.\n",
    "\n",
    "Let's use the Google vocab and add punctuations and contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "# Extend common vocab to include punctuation + contractions\n",
    "from string import punctuation\n",
    "vocab = google_most_common + list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text and authors to network-ready input\n",
    "### Download embedding model to represent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# https:/github.com/RaRe-Technologies/gensim-data\n",
    "# glove-twitter-25\n",
    "# word2vec-google-news-300\n",
    "info = api.info()\n",
    "embed_model = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'house':\n",
      " [-1.3345e-01  3.4688e-01  3.0748e-01 -2.1794e-03  7.1898e-01 -2.8725e-03\n",
      "  9.5989e-02  5.5276e-01  1.2153e-01 -2.6555e-01 -1.0277e+00  7.2278e-01\n",
      " -4.2767e+00 -9.0406e-02  1.1909e-01 -5.0647e-02 -3.3165e-01 -1.8213e-01\n",
      " -3.6218e-01  6.9813e-03  2.0147e-01 -2.9150e-01 -1.6417e-01 -2.8022e-01\n",
      "  5.4800e-01 -5.8081e-01  3.8146e-01 -5.5519e-01  1.6094e-01 -5.2039e-02\n",
      " -1.4798e-01  1.0892e-03 -2.6702e-01 -1.7885e-01  5.1449e-02  6.7434e-02\n",
      "  9.5654e-02  5.6137e-01  7.1208e-03  4.7000e-01 -3.1460e-01  1.0552e+00\n",
      "  5.2215e-01 -4.8432e-01  2.8615e-01  7.9474e-02  6.4211e-01  6.5274e-01\n",
      " -2.6493e-01 -8.9566e-02 -2.6298e-01 -3.4906e-01  3.3645e-02  2.1278e-01\n",
      " -1.0738e+00 -3.6867e-01  1.8473e-01  3.3821e-01  5.7516e-01  1.7559e-01\n",
      " -1.5436e-01  5.2836e-02 -9.8523e-02 -4.0975e-01 -8.5839e-02 -3.1527e-01\n",
      "  1.7936e-01 -2.0953e-01  6.6424e-01 -5.7412e-02  2.4528e-01 -2.2577e-01\n",
      " -3.3233e-01  2.1225e-01  2.3743e-01  1.3298e-01 -4.4889e-01  4.9577e-01\n",
      "  4.3360e-01  2.4248e-01  1.6624e+00  4.2981e-01 -4.8961e-01 -2.3809e-01\n",
      "  1.6583e-01 -4.9037e-01  3.6121e-01  8.0868e-01  5.0630e-01 -6.9646e-02\n",
      " -5.2503e-01 -7.9513e-03  5.3885e-01 -7.6658e-02 -2.5745e-01  6.0910e-01\n",
      "  4.5299e-01 -3.2974e-01 -5.1177e-01 -2.7013e-01]\n",
      "\n",
      "Similar words to 'house':\n",
      " [('room', 0.8465880155563354), ('home', 0.8294692635536194), ('party', 0.7633090019226074), ('going', 0.7493972778320312), ('out', 0.7491909861564636), ('office', 0.747423529624939), ('now', 0.7471632957458496), ('apartment', 0.7466973662376404), ('up', 0.7463924884796143), ('family', 0.743867039680481)]\n"
     ]
    }
   ],
   "source": [
    "# print sample data\n",
    "print(\"Embedding for 'house':\\n\", embed_model.wv['house'])\n",
    "print(\"\\nSimilar words to 'house':\\n\", embed_model.most_similar(\"house\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded vector representation of some words of the first text:\n",
      " [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.25888  ,  0.20283  ,  0.45292  ,  0.051316 , -0.59831  ,\n",
      "        0.17218  ,  0.43064  , -0.47245  , -0.11205  , -0.51921  ,\n",
      "        0.20608  ,  0.33019  , -2.7551   , -0.28471  ,  0.78403  ,\n",
      "        0.95814  ,  0.74391  , -0.40147  , -0.052915 , -0.38103  ,\n",
      "       -0.80142  , -0.2495   ,  0.12538  , -0.29809  ,  0.28059  ,\n",
      "        0.25785  , -0.2339   ,  0.7638   , -0.040205 ,  0.017227 ,\n",
      "        0.72153  ,  0.023785 ,  0.80918  , -0.095793 ,  0.44504  ,\n",
      "       -0.47478  ,  0.47513  ,  0.49704  ,  0.48058  , -0.010213 ,\n",
      "        0.32494  , -0.14895  , -1.3635   , -0.17402  ,  0.78538  ,\n",
      "        0.81082  , -0.33958  ,  0.79257  ,  0.83707  ,  0.54508  ,\n",
      "        0.14719  , -0.13767  , -0.19193  , -0.079262 ,  0.053628 ,\n",
      "        0.55179  ,  0.40727  ,  0.46086  ,  0.023048 , -0.30633  ,\n",
      "       -0.35609  ,  0.12552  ,  0.26406  , -0.34549  , -0.25536  ,\n",
      "       -0.66565  , -0.47676  , -0.48666  ,  0.070679 , -0.33258  ,\n",
      "       -0.10105  , -0.80394  , -0.56178  , -0.027234 , -0.36882  ,\n",
      "        0.37542  , -0.49345  ,  0.088327 ,  0.51967  , -0.076404 ,\n",
      "        1.7169   ,  0.053974 , -0.58804  ,  0.52677  ,  0.26568  ,\n",
      "       -0.40018  ,  0.09395  ,  0.37298  , -0.0061981,  0.29486  ,\n",
      "       -0.17694  , -0.15929  ,  0.526    , -0.17187  ,  0.55928  ,\n",
      "        0.65577  , -0.17623  , -0.9905   , -0.37194  , -0.10695  ],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "# convert text to vectors based on frequency\n",
    "embed_size = embed_model.vector_size\n",
    "embed_zeros = np.zeros(embed_size)\n",
    "\n",
    "for i, doc in enumerate(X):\n",
    "    X[i] = [embed_model[word] \\\n",
    "                    if word in vocab and word in embed_model else embed_zeros \\\n",
    "                    for word in doc]\n",
    "\n",
    "print(\"Embedded vector representation of some words of the first text:\\n\", X[0][0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "Creating smaller windows of data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100 word chucks\n",
    "WINDOW_SIZE = 300\n",
    "WINDOW_SPACING = 100\n",
    "\n",
    "def chunk(x, y):\n",
    "    X_chunk = []\n",
    "    y_chunk = []\n",
    "    \n",
    "    for i in range(0, len(x)-WINDOW_SIZE, WINDOW_SPACING):\n",
    "        X_chunk.append(x[i:i+WINDOW_SIZE])\n",
    "        y_chunk.append(y)\n",
    "    \n",
    "    return X_chunk, y_chunk\n",
    "\n",
    "X_chunks = []\n",
    "y_chunks = []\n",
    "for i, _ in enumerate(X):\n",
    "    xc, yc = chunk(X[i], y[i])\n",
    "    X_chunks += xc\n",
    "    y_chunks += yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_chunks shape: (16545, 300, 100)\n",
      "y_chunks shape: (16545,)\n"
     ]
    }
   ],
   "source": [
    "X_chunks = np.array(X_chunks)\n",
    "y_chunks = np.array(y_chunks)\n",
    "print(\"X_chunks shape:\", X_chunks.shape)\n",
    "print(\"y_chunks shape:\", y_chunks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training, test, and \"new\" sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New:   3407 text from 10 authors\n",
      "Train: 11824 text from 40 authors\n",
      "Test:  1314 text from 40 authors\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keeps some authors aside for hash testing\n",
    "x_train, x_new, y_train, y_new = train_test_split(X_chunks, y_chunks, train_size=13138, shuffle=False)\n",
    "\n",
    "# Split remainder into 70% training and 30% testing and shuffle\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, train_size=0.9, random_state=1)\n",
    "\n",
    "print(\"New:   {} text from {} authors\".format(x_new.shape[0], len(np.unique(y_new, axis=0))))\n",
    "print(\"Train: {} text from {} authors\".format(x_train.shape[0], len(np.unique(y_train, axis=0))))\n",
    "print(\"Test:  {} text from {} authors\".format(x_test.shape[0], len(np.unique(y_test, axis=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode labels (authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author RobinSidel is one-hot encoded as:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#encoder = LabelEncoder()\n",
    "#encoded = encoder.fit_transform(df[\"author\"])\n",
    "#y = to_categorical(encoded)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded = encoder.fit_transform(y_train)\n",
    "y_train = to_categorical(encoded)\n",
    "y_test = to_categorical(encoder.transform(y_test))\n",
    "\n",
    "print(\"Author {} is one-hot encoded as:\\n\".format(df[\"author\"][0]), y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-b3e48bfc0d7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/inputs.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "obj = [x_train, x_test, x_new, y_train, y_test, y_new, embed_size]\n",
    "obj = [x_train]\n",
    "with open(PROCESSED_DATA_PATH, \"wb\") as handle:\n",
    "    pickle.dump(x_train, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del X \n",
    "del y \n",
    "del embed_model \n",
    "del df\n",
    "del X_chunks\n",
    "del y_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TOKENIZER, 'rb') as handle:\n",
    "                tokenizer = pickle.load(handle)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(LSTM(128, dropout=0.2, input_shape=(WINDOW_SIZE, embed_size), return_sequences=True))\n",
    "model.add(LSTM(64, dropout=0.2, input_shape=(WINDOW_SIZE, embed_size)))\n",
    "model.add(Dense(40, activation='softmax', name='output'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9459 samples, validate on 2365 samples\n",
      "Epoch 1/25\n",
      "9459/9459 [==============================] - 84s 9ms/step - loss: 3.6341 - acc: 0.0383 - val_loss: 3.4819 - val_acc: 0.0558\n",
      "Epoch 2/25\n",
      "9459/9459 [==============================] - 85s 9ms/step - loss: 3.3630 - acc: 0.0777 - val_loss: 3.6563 - val_acc: 0.0562\n",
      "Epoch 3/25\n",
      "9459/9459 [==============================] - 80s 8ms/step - loss: 3.3905 - acc: 0.0679 - val_loss: 3.4283 - val_acc: 0.0677\n",
      "Epoch 4/25\n",
      "9459/9459 [==============================] - 88s 9ms/step - loss: 3.3288 - acc: 0.0901 - val_loss: 3.1510 - val_acc: 0.1230\n",
      "Epoch 5/25\n",
      "9459/9459 [==============================] - 84s 9ms/step - loss: 3.3256 - acc: 0.0813 - val_loss: 3.2789 - val_acc: 0.0757\n",
      "Epoch 6/25\n",
      "9459/9459 [==============================] - 81s 9ms/step - loss: 3.2088 - acc: 0.1032 - val_loss: 3.0523 - val_acc: 0.1302\n",
      "Epoch 7/25\n",
      "9459/9459 [==============================] - 84s 9ms/step - loss: 3.0828 - acc: 0.1177 - val_loss: 2.9382 - val_acc: 0.1543\n",
      "Epoch 8/25\n",
      "9459/9459 [==============================] - 76s 8ms/step - loss: 3.0896 - acc: 0.1142 - val_loss: 2.9839 - val_acc: 0.1302\n",
      "Epoch 9/25\n",
      "9459/9459 [==============================] - 76s 8ms/step - loss: 2.9946 - acc: 0.1302 - val_loss: 2.8556 - val_acc: 0.1590\n",
      "Epoch 10/25\n",
      "9459/9459 [==============================] - 79s 8ms/step - loss: 3.1301 - acc: 0.1273 - val_loss: 3.0084 - val_acc: 0.1459\n",
      "Epoch 11/25\n",
      "9459/9459 [==============================] - 81s 9ms/step - loss: 2.9470 - acc: 0.1533 - val_loss: 3.2168 - val_acc: 0.1108\n",
      "Epoch 12/25\n",
      "9459/9459 [==============================] - 97s 10ms/step - loss: 2.8970 - acc: 0.1600 - val_loss: 2.7548 - val_acc: 0.1624\n",
      "Epoch 13/25\n",
      "9459/9459 [==============================] - 92s 10ms/step - loss: 2.7851 - acc: 0.1715 - val_loss: 2.7276 - val_acc: 0.1734\n",
      "Epoch 14/25\n",
      "9459/9459 [==============================] - 93s 10ms/step - loss: 2.6603 - acc: 0.1891 - val_loss: 2.6296 - val_acc: 0.1983\n",
      "Epoch 15/25\n",
      "9088/9459 [===========================>..] - ETA: 3s - loss: 2.6968 - acc: 0.1903"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-2ff971d70957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0;31m#callbacks=[logger],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           shuffle=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/oa/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/oa/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/oa/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/oa/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RUN_NAME = \"run 3 with LSTM 64\"\n",
    "logger = keras.callbacks.TensorBoard(\n",
    "    log_dir='logs/{}'.format(RUN_NAME),\n",
    "    write_graph=True,\n",
    "    histogram_freq=5\n",
    ")\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          batch_size=128,\n",
    "          epochs=25,\n",
    "          validation_split=0.2,\n",
    "          #callbacks=[logger],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(x_test, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"data/5-lstm64-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
