{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept\n",
    "\n",
    "A proof-of-concept of the main \"originality score\" algorithm: preprocessing a sample paper, performing analytics, saving the document's hash, and returning a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataset\n",
    "Load the [Reuter 50_50 training dataset](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50).\n",
    "\n",
    "TODO:  download and extract directly from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Drugstore giant Revco D.S. Inc. said Monday it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Mattel Inc., seeking to expand in the market f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>A financial agreement between Barney's Inc and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>An independent shareholder advisory firm recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RobinSidel</td>\n",
       "      <td>Raising the stakes in the escalating battle fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text\n",
       "0  RobinSidel  Drugstore giant Revco D.S. Inc. said Monday it...\n",
       "1  RobinSidel  Mattel Inc., seeking to expand in the market f...\n",
       "2  RobinSidel  A financial agreement between Barney's Inc and...\n",
       "3  RobinSidel  An independent shareholder advisory firm recom...\n",
       "4  RobinSidel  Raising the stakes in the escalating battle fo..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source modified from:\n",
    "# https://github.com/devanshdalal/Author-Identification-task/blob/master/learner.py\n",
    "path = 'data/C50/C50train/'\n",
    "authors = os.listdir(path)\n",
    "data = []\n",
    "\n",
    "for author in authors:\n",
    "  texts = os.listdir(path + author + '/')\n",
    "  for text in texts:\n",
    "    f=open(path + author + '/' + text, 'r')\n",
    "    data.append([author, f.read()])\n",
    "    f.close()\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"author\", \"text\"])\n",
    "df.head()\n",
    "\n",
    "# TODO: add more author, text pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "### Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=10000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(df[\"text\"])\n",
    "X = tokenizer.texts_to_sequences(df[\"text\"])\n",
    "\n",
    "# plot histogram \n",
    "num_words = [len(n) for n in X]\n",
    "plt.hist(num_words, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save Tokenizers\n",
    "with open('data/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author RobinSidel is one-hot encoded as:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded = encoder.fit_transform(df[\"author\"])\n",
    "y = to_categorical(encoded)\n",
    "\n",
    "print(\"Author {} is one-hot encoded as:\\n\".format(df[\"author\"][0]), y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100 word chucks\n",
    "CHUNK_SIZE = 100\n",
    "X_chunks = []\n",
    "y_chunks = []\n",
    "\n",
    "def chunk(x, y):\n",
    "    x_chunk = []\n",
    "    y_chunk = []\n",
    "    \n",
    "    for i in range(0, len(x)-CHUNK_SIZE, 20):\n",
    "        x_chunk.append(x[i:i+CHUNK_SIZE])\n",
    "        y_chunk.append(y)\n",
    "    \n",
    "    return x_chunk, y_chunk\n",
    "\n",
    "for i, _ in enumerate(X):\n",
    "    xc, yc = chunk(X[i], y[i])\n",
    "    X_chunks += xc\n",
    "    y_chunks += yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50557"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_chunks = np.array(X_chunks)\n",
    "y_chunks = np.array(y_chunks)\n",
    "len(X_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the first text tokenized, size 691:\n",
      " [7482, 584, 4011, 2324, 48, 109, 7, 198, 12, 433, 2, 287, 528, 1363, 201, 682, 109, 5, 4, 8974, 535, 1690, 15, 4978, 30, 1, 987, 759, 8, 2265, 95, 4011, 2, 287, 90, 1847, 84, 3, 201, 682, 1117, 80, 8, 505, 319, 203, 64, 40, 20, 7483, 224, 3, 235, 203, 64, 31, 201, 682, 1548, 55, 184, 37, 29, 126, 5319, 52, 1, 1743, 3, 4011, 6, 201, 682, 76, 1638, 1978, 37, 42, 543, 2, 988, 34, 693, 2, 4, 971, 6, 1339, 3845, 7, 4012, 155, 6, 116, 134, 668, 3, 4011, 1, 107, 22, 4591, 1, 1167, 176, 6, 5955, 217, 620, 1069, 3754, 1, 70, 7, 201, 8975, 367, 3, 1340, 5154, 999, 1, 673, 224, 6, 2964, 10, 201, 682, 278, 2225, 58, 84, 1, 492, 3, 95, 201, 682, 2496, 1, 673, 454, 5, 1, 1766, 5155, 7482, 71, 4711, 1959, 68, 1079, 33, 333, 34, 184, 2, 287, 1019, 1352, 1363, 4712, 6780, 109, 8, 39, 114, 44, 6, 620, 109, 4, 353, 3, 1593, 213, 5956, 218, 109, 433, 5, 608, 2, 287, 109, 8, 52, 6498, 30, 397, 201, 682, 1548, 1, 1219, 224, 17, 80, 572, 35, 46, 180, 15, 52, 628, 203, 64, 9, 1864, 10, 1, 41, 27, 14, 694, 8, 47, 57, 1881, 235, 203, 64, 224, 80, 1297, 40, 307, 89, 272, 15, 1133, 216, 4, 64, 9, 1, 36, 393, 80, 250, 201, 8975, 84, 42, 15, 505, 40, 1487, 565, 272, 5, 1364, 180, 9, 1316, 13, 112, 860, 4841, 2325, 6, 462, 6, 87, 6499, 37, 115, 868, 1032, 120, 933, 288, 954, 764, 6, 256, 120, 87, 470, 391, 72, 5156, 34, 1743, 22, 579, 1, 933, 41, 2, 2096, 264, 59, 4, 1432, 764, 3, 861, 7, 201, 682, 7, 12, 22, 311, 13, 4011, 2, 1506, 4, 3542, 1199, 234, 171, 72, 528, 7482, 3754, 201, 682, 24, 46, 2640, 523, 10, 25, 5157, 1, 6500, 4592, 3, 1708, 719, 2139, 70, 6, 582, 2266, 1578, 12, 24, 46, 1691, 16, 123, 801, 6, 87, 6, 17, 156, 25, 1940, 645, 558, 71, 1099, 191, 4011, 11, 197, 2326, 2, 5744, 125, 5, 1, 3543, 16, 546, 1381, 3, 201, 8975, 1278, 739, 6, 1767, 6781, 861, 5, 5745, 4011, 22, 14, 543, 2, 5320, 201, 682, 676, 1208, 7, 4842, 3, 4979, 518, 74, 201, 8975, 123, 801, 814, 2, 7863, 55, 28, 294, 2, 5746, 5, 7864, 1, 701, 3, 273, 6211, 137, 775, 16, 1708, 2139, 1382, 397, 12, 1780, 5, 4, 36, 195, 239, 2, 389, 1507, 123, 801, 118, 3671, 13, 1, 41, 25, 7, 17, 1960, 3486, 1278, 739, 11, 284, 15, 141, 509, 26, 3, 689, 54, 7, 45, 3, 63, 2097, 22, 14, 2, 5511, 1069, 5747, 913, 5, 5745, 229, 4011, 2195, 3231, 861, 6, 201, 682, 24, 2747, 861, 633, 43, 265, 1, 1278, 6, 1, 861, 868, 12, 22, 14, 4, 530, 123, 739, 8, 4011, 7, 610, 5158, 3, 185, 2793, 301, 5158, 112, 74, 4011, 49, 7, 12, 22, 5748, 6, 2748, 17, 2225, 224, 8, 201, 682, 84, 362, 2920, 235, 1, 224, 35, 46, 211, 2, 7865, 198, 1, 61, 70, 49, 7, 90, 3357, 136, 214, 22, 14, 5512, 53, 60, 7866, 52, 498, 4011, 228, 34, 184, 1079, 4, 5321, 333, 13, 201, 682, 619, 4, 15, 17, 2466, 6, 3544, 6782, 157, 12, 751, 1, 201, 682, 367, 1289, 59, 1, 1455, 6, 2166, 1, 36, 6, 244, 224, 20, 4011, 685, 3, 72, 1104, 42, 32, 1424, 641, 397, 68, 19, 1979, 18, 4, 470, 3004, 347, 2, 442, 201, 682, 109, 2195, 1209, 1725, 1, 7867, 131, 163, 4011, 2195, 66, 4843, 861, 5, 378, 7867, 6, 2140, 163, 1, 107, 13, 201, 682, 19, 344, 18, 4, 731, 454, 8, 4011, 31, 228, 34, 28, 19, 927, 5, 4, 928, 987, 13, 4711, 1959, 1, 61, 70, 35, 1079, 4, 1210, 8, 4711, 1959, 2, 287, 4011, 8, 39, 161, 44, 23, 1, 107, 3927, 53, 843, 1606, 1053, 10, 1, 441, 27, 746, 425, 137]\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences, use 500 as maximum length.\n",
    "#X = pad_sequences(X, maxlen=500)\n",
    "\n",
    "print(\"Here is the first text tokenized, size {}:\\n\".format(len(X[0])), X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugstore\n",
      "giant\n",
      "revco\n",
      "d\n",
      "s\n",
      "inc\n",
      "said\n"
     ]
    }
   ],
   "source": [
    "# Break sequences into chuncks\n",
    "inv_map = {v: k for k, v in tokenizer.word_index.items()}\n",
    "for i in [7482, 584, 4011, 2324, 48, 109, 7]:\n",
    "    print(inv_map[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network\n",
    "### Create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New:  10112 text from 10 authors\n",
      "Train: 28311 text from 41 authors\n",
      "Test:  12134 text from 41 authors\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keeps some authors aside for hash testing\n",
    "x_train, x_new, y_train, y_new = train_test_split(X_chunks, y_chunks, train_size=0.8, shuffle=False)\n",
    "\n",
    "# Split remainder into 70% training and 30% testing and shuffle\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, train_size=0.7, random_state=1)\n",
    "\n",
    "print(\"New:  {} text from {} authors\".format(x_new.shape[0], len(np.unique(y_new, axis=0))))\n",
    "print(\"Train: {} text from {} authors\".format(x_train.shape[0], len(np.unique(y_train, axis=0))))\n",
    "print(\"Test:  {} text from {} authors\".format(x_test.shape[0], len(np.unique(y_test, axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data, showing authors\n",
      "[10 34 22  5 29 30 18 36 41 13  3 36 33 45 36  9 14 45 41 10 38  8 23 14\n",
      " 19 12 46 14 46 14 23 36  9 41 40 14 21 43 30 41  3 34  6 18  1 28  8 21\n",
      " 28  6 31 45 19 16 41 35 28 19 12 15 49 40 36 12 29 30 14 36 39 43 45 21\n",
      "  0  0 33 27  9  0 41 14 36 49 44 21 46 23 29 38  7  6 31 35 35 17 12 35\n",
      " 23 28 13 12]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample training data, showing authors\")\n",
    "print(np.argmax(y_train, axis=1)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, None, 128)         1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 200)         183200    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                10050     \n",
      "=================================================================\n",
      "Total params: 1,714,050\n",
      "Trainable params: 1,714,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, SimpleRNN, RepeatVector, TimeDistributed, Bidirectional\n",
    "\n",
    "RUN_NAME = \"run 4 with LSTM 128, embed 128, batch_size=10\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 128, mask_zero=True, name='embed'))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3, return_sequences=True, name='lstm')))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3, return_sequences=False, name='lstm2')))\n",
    "model.add(Dense(50, activation='softmax', name='dense'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = keras.callbacks.TensorBoard(\n",
    "    log_dir='logs/{}'.format(RUN_NAME),\n",
    "    write_graph=True,\n",
    "    histogram_freq=5\n",
    ")\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_train,\n",
    "#          batch_size=2000,\n",
    "          epochs=15,\n",
    "          validation_split=0.2,\n",
    "#          callbacks=[logger],\n",
    "          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'data/2-lstm100-model.h5'\n",
    "model = keras.models.load_model(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12134/12134 [==============================] - 38s 3ms/step\n",
      "Test score: 0.3537454803478699\n",
      "Test accuracy: 0.8880006593044338\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compare hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_author(index):\n",
    "    return encoder.inverse_transform(y_hash[index])\n",
    "\n",
    "def get_hash(text):\n",
    "    return model.predict(text)\n",
    "\n",
    "def get_similarity(hash1, hash2):\n",
    "    return cosine_similarity(hash1, hash2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hash = get_hash(x_new)\n",
    "y_hash = [np.argmax(y_hot) for y_hot in y_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparision of text 0 and 0 \tfor same author MatthewBunce is: \t\t 1.0000001192\n",
      "Comparision of text 0 and 100 \tfor same author MatthewBunce is: \t\t 0.6966342926\n",
      "Comparision of text 0 and 200 \tfor same author MatthewBunce is: \t\t 0.3324630857\n",
      "Comparision of text 0 and 300 \tfor same author MatthewBunce is: \t\t 0.6608810425\n",
      "Comparision of text 0 and 400 \tfor same author MatthewBunce is: \t\t 0.6722244024\n",
      "Comparision of text 0 and 500 \tfor same author MatthewBunce is: \t\t 0.7165437341\n",
      "Comparision of text 0 and 600 \tfor same author MatthewBunce is: \t\t 0.1321511269\n",
      "Comparision of text 0 and 700 \tfor same author MatthewBunce is: \t\t 0.6805967093\n",
      "Comparision of text 0 and 800 \tfor same author MatthewBunce is: \t\t 0.1318429708\n",
      "Comparision of text 0 and 900 \tfor authors MatthewBunce and ToddNissen is: \t 0.0056230682\n",
      "Comparision of text 0 and 1000 \tfor authors MatthewBunce and ToddNissen is: \t 0.0000740755\n",
      "Comparision of text 0 and 1100 \tfor authors MatthewBunce and ToddNissen is: \t 0.0000591688\n",
      "Comparision of text 0 and 1200 \tfor authors MatthewBunce and ToddNissen is: \t 0.0000147494\n",
      "Comparision of text 0 and 1300 \tfor authors MatthewBunce and ToddNissen is: \t 0.2077665329\n",
      "Comparision of text 0 and 1400 \tfor authors MatthewBunce and ToddNissen is: \t 0.0053635063\n",
      "Comparision of text 0 and 1500 \tfor authors MatthewBunce and ToddNissen is: \t 0.0000398659\n",
      "Comparision of text 0 and 1600 \tfor authors MatthewBunce and ToddNissen is: \t 0.0000792754\n",
      "Comparision of text 0 and 1700 \tfor authors MatthewBunce and ToddNissen is: \t 0.0001301791\n",
      "Comparision of text 0 and 1800 \tfor authors MatthewBunce and ToddNissen is: \t 0.0000436416\n",
      "Comparision of text 0 and 1900 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.1332753450\n",
      "Comparision of text 0 and 2000 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.0004486299\n",
      "Comparision of text 0 and 2100 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.0003053194\n",
      "Comparision of text 0 and 2200 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.0003667334\n",
      "Comparision of text 0 and 2300 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.0015581148\n",
      "Comparision of text 0 and 2400 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.0007724207\n",
      "Comparision of text 0 and 2500 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.0146654705\n",
      "Comparision of text 0 and 2600 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.1302827895\n",
      "Comparision of text 0 and 2700 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.0015722200\n",
      "Comparision of text 0 and 2800 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.0233077630\n",
      "Comparision of text 0 and 2900 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.0009789494\n",
      "Comparision of text 0 and 3000 \tfor authors MatthewBunce and PeterHumphrey is: \t 0.0306134503\n",
      "Comparision of text 0 and 3100 \tfor authors MatthewBunce and TimFarrand is: \t 0.0835975930\n",
      "Comparision of text 0 and 3200 \tfor authors MatthewBunce and TimFarrand is: \t 0.0023249921\n",
      "Comparision of text 0 and 3300 \tfor authors MatthewBunce and TimFarrand is: \t 0.0001289981\n",
      "Comparision of text 0 and 3400 \tfor authors MatthewBunce and TimFarrand is: \t 0.0004290142\n",
      "Comparision of text 0 and 3500 \tfor authors MatthewBunce and TimFarrand is: \t 0.0224412400\n",
      "Comparision of text 0 and 3600 \tfor authors MatthewBunce and TimFarrand is: \t 0.0056863148\n",
      "Comparision of text 0 and 3700 \tfor authors MatthewBunce and TimFarrand is: \t 0.0819523335\n",
      "Comparision of text 0 and 3800 \tfor authors MatthewBunce and TimFarrand is: \t 0.0811751708\n",
      "Comparision of text 0 and 3900 \tfor authors MatthewBunce and TimFarrand is: \t 0.0004885493\n",
      "Comparision of text 0 and 4000 \tfor authors MatthewBunce and TimFarrand is: \t 0.0140163368\n",
      "Comparision of text 0 and 4100 \tfor authors MatthewBunce and SarahDavison is: \t 0.0479133539\n",
      "Comparision of text 0 and 4200 \tfor authors MatthewBunce and SarahDavison is: \t 0.1334952563\n",
      "Comparision of text 0 and 4300 \tfor authors MatthewBunce and SarahDavison is: \t 0.0015660191\n",
      "Comparision of text 0 and 4400 \tfor authors MatthewBunce and SarahDavison is: \t 0.7901577353\n",
      "Comparision of text 0 and 4500 \tfor authors MatthewBunce and SarahDavison is: \t 0.4801317155\n",
      "Comparision of text 0 and 4600 \tfor authors MatthewBunce and SarahDavison is: \t 0.0133290403\n",
      "Comparision of text 0 and 4700 \tfor authors MatthewBunce and SarahDavison is: \t 0.0575788841\n",
      "Comparision of text 0 and 4800 \tfor authors MatthewBunce and SarahDavison is: \t 0.1334926933\n",
      "Comparision of text 0 and 4900 \tfor authors MatthewBunce and SarahDavison is: \t 0.0007068770\n",
      "Comparision of text 0 and 5000 \tfor authors MatthewBunce and SarahDavison is: \t 0.0102811698\n",
      "Comparision of text 0 and 5100 \tfor authors MatthewBunce and SarahDavison is: \t 0.0125074144\n",
      "Comparision of text 0 and 5200 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0000614363\n",
      "Comparision of text 0 and 5300 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0801606402\n",
      "Comparision of text 0 and 5400 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.1352002770\n",
      "Comparision of text 0 and 5500 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0919019505\n",
      "Comparision of text 0 and 5600 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0193386339\n",
      "Comparision of text 0 and 5700 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0291854311\n",
      "Comparision of text 0 and 5800 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0130947763\n",
      "Comparision of text 0 and 5900 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.1402806193\n",
      "Comparision of text 0 and 6000 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.1033901423\n",
      "Comparision of text 0 and 6100 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0571742468\n",
      "Comparision of text 0 and 6200 \tfor authors MatthewBunce and GrahamEarnshaw is: \t 0.0092702145\n",
      "Comparision of text 0 and 6300 \tfor authors MatthewBunce and BernardHickey is: \t 0.0008356673\n",
      "Comparision of text 0 and 6400 \tfor authors MatthewBunce and BernardHickey is: \t 0.0003447102\n",
      "Comparision of text 0 and 6500 \tfor authors MatthewBunce and BernardHickey is: \t 0.0004801326\n",
      "Comparision of text 0 and 6600 \tfor authors MatthewBunce and BernardHickey is: \t 0.0000108979\n",
      "Comparision of text 0 and 6700 \tfor authors MatthewBunce and BernardHickey is: \t 0.0026776278\n",
      "Comparision of text 0 and 6800 \tfor authors MatthewBunce and BernardHickey is: \t 0.0098680295\n",
      "Comparision of text 0 and 6900 \tfor authors MatthewBunce and BernardHickey is: \t 0.1292482167\n",
      "Comparision of text 0 and 7000 \tfor authors MatthewBunce and BernardHickey is: \t 0.0000707192\n",
      "Comparision of text 0 and 7100 \tfor authors MatthewBunce and BernardHickey is: \t 0.0000094856\n",
      "Comparision of text 0 and 7200 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0001809208\n",
      "Comparision of text 0 and 7300 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0001272066\n",
      "Comparision of text 0 and 7400 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0003486170\n",
      "Comparision of text 0 and 7500 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0003299101\n",
      "Comparision of text 0 and 7600 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0015284603\n",
      "Comparision of text 0 and 7700 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0000882212\n",
      "Comparision of text 0 and 7800 \tfor authors MatthewBunce and KirstinRidley is: \t 0.2608187199\n",
      "Comparision of text 0 and 7900 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0001165678\n",
      "Comparision of text 0 and 8000 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0011199862\n",
      "Comparision of text 0 and 8100 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0005892505\n",
      "Comparision of text 0 and 8200 \tfor authors MatthewBunce and KirstinRidley is: \t 0.0000358033\n",
      "Comparision of text 0 and 8300 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0005002341\n",
      "Comparision of text 0 and 8400 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0966225341\n",
      "Comparision of text 0 and 8500 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0001708677\n",
      "Comparision of text 0 and 8600 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0002652933\n",
      "Comparision of text 0 and 8700 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0004430346\n",
      "Comparision of text 0 and 8800 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0226204675\n",
      "Comparision of text 0 and 8900 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0000361903\n",
      "Comparision of text 0 and 9000 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0814263895\n",
      "Comparision of text 0 and 9100 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0002185363\n",
      "Comparision of text 0 and 9200 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0224878471\n",
      "Comparision of text 0 and 9300 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0042548743\n",
      "Comparision of text 0 and 9400 \tfor authors MatthewBunce and AlexanderSmith is: \t 0.0785002932\n",
      "Comparision of text 0 and 9500 \tfor authors MatthewBunce and LydiaZajc is: \t 0.0000842232\n",
      "Comparision of text 0 and 9600 \tfor authors MatthewBunce and LydiaZajc is: \t 0.0000931638\n",
      "Comparision of text 0 and 9700 \tfor authors MatthewBunce and LydiaZajc is: \t 0.0002495103\n",
      "Comparision of text 0 and 9800 \tfor authors MatthewBunce and LydiaZajc is: \t 0.0002553391\n",
      "Comparision of text 0 and 9900 \tfor authors MatthewBunce and LydiaZajc is: \t 0.0907817632\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_similarity(i, j):\n",
    "    similarity = get_similarity([x_hash[i]], [x_hash[j]])\n",
    "    \n",
    "    if get_author(i) == get_author(j):\n",
    "        print(\"Comparision of text {} and {} \\tfor same author {} is: \\t\\t\".format(\n",
    "             i, j, get_author(i)), end=' ')\n",
    "        print(\"{:0.10f}\".format(similarity))\n",
    "    else:\n",
    "        print(\"Comparision of text {} and {} \\tfor authors {} and {} is: \\t\".format(\n",
    "             i, j, get_author(i), get_author(j)), end=' ')\n",
    "        print(\"{:0.10f}\".format(similarity))\n",
    "\n",
    "for i in range(0, 10000, 100):\n",
    "    print_similarity(0, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "true_positive, true_negative, false_positive, false_negative = 0,0,0,0\n",
    "margin = 0.2\n",
    "num_texts = len(x_new)\n",
    "\n",
    "for i in range(num_texts):\n",
    "    similarity = get_similarity([x_hash[i]], x_hash)\n",
    "    \n",
    "    for j in range(num_texts):\n",
    "        if similarity[0][j] >= margin:\n",
    "            if y_hash[i] == y_hash[j]:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "        else:\n",
    "            if y_hash[i] == y_hash[j]:\n",
    "                false_negative += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "\n",
    "print(\"True positives \", true_positive)\n",
    "print(\"False positives\", false_positive)\n",
    "print(\"True negatives \", true_negative)\n",
    "print(\"False negatives\", false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-3e283c2fdf3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_hash\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-a472ddbd7905>\u001b[0m in \u001b[0;36mget_similarity\u001b[0;34m(hash1, hash2)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# Comparison just for the same author\n",
    "y_new_ints = np.unique(np.argmax(y_new, axis=1))\n",
    "y_new_authors = encoder.inverse_transform(y_new_ints)\n",
    "new_authors = {name:{\"correct\":0, \"incorrect\":0} for name in y_new_authors}\n",
    "\n",
    "for i in range(num_texts):\n",
    "    similarity = get_similarity([x_hash[i]], x_hash)\n",
    "    \n",
    "    for j in range(num_texts):\n",
    "        if y_hash[i] == y_hash[j]:\n",
    "            if similarity[0][j] >= margin:\n",
    "                new_authors[get_author(i)][\"correct\"] += 1\n",
    "            else:\n",
    "                new_authors[get_author(i)][\"incorrect\"] += 1\n",
    "\n",
    "print(\"Number of correctly identified text belonging to each author:\")             \n",
    "new_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1945, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[new_authors[d][\"correct\"] for d in new_authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x=y_new_authors,\n",
    "    y=[new_authors[d][\"correct\"] for d in new_authors],\n",
    "    name='Correct'\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x=y_new_authors,\n",
    "    y=[new_authors[d][\"incorrect\"] for d in new_authors],\n",
    "    name='Incorrect'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    title='Hash correctness using Margin=0.8',\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='jupyter-basic_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The chart above shows that the neural network does a great job of identifying the text written by Lydia and Todd and a good job identifying the text from Graham, Matthew, and Peter. For the other authors, it does not perform as well; in two cases (Alexander and Kristin) it is wrong more than it is right. Another issue is the large percentage of false positives when compared to other author texts. While some text from different authors may share certain characteristics, the goal of the algorithm is to maximize their differences.\n",
    "\n",
    "More work will be coming in two areas: (1) improving the network to have > 80% test accuracy (if possible) and improving the comparison algorithm's ability to differentiate texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#model.save(\"data/2-lstm100-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
